{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5ce80f",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4. Использование нейронных сетей для генерации текста\n",
    "# Козлов, Ярикова\n",
    "# Вариант №9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a8c26",
   "metadata": {},
   "source": [
    "### Код для быстрой записи данных в таблицы (к лабе не относится)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae1f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "class Table():\n",
    "    def __init__(self, title=None, data=None, headlines=[\"Precision\", \"Recall\", \"F1-score\", \"Accuracy\"],\n",
    "                 indexes=[\"Стемминг отсутствует\", \"Стемминг есть\",\n",
    "                          \"Стоп-слова есть\", \"Стоп-слова отсечены\",\n",
    "                       \"Взвешивание Count\", \"Взвешивание TF\", \"Взвешивание TF-IDF\",\n",
    "                       \"max_features 100\", \"max_features 1000\", \"max_features 2000\", \"max_features 5000\", \"max_features 10000\"],\n",
    "                weighted=True):\n",
    "        \n",
    "        self.headlines = headlines\n",
    "        self.dataFrame = pd.DataFrame(columns=headlines, index=indexes)\n",
    "        if weighted:\n",
    "            self.dataFrame.columns = pd.MultiIndex.from_tuples(\n",
    "                tuple(zip(('Weighted Avg', 'Weighted Avg', 'Weighted Avg', 'Weighted Avg'), self.dataFrame.columns)))\n",
    "        \n",
    "    def fill_row(self, row, data=[], pred='', target='', roc_auc=None):\n",
    "        \"\"\"\n",
    "        Заполняет строки исходя из дефолта\n",
    "        row - row index\n",
    "        pred='' и target='' - полученные классификатором значения и целевые значения\n",
    "        Заполняется значениями Weighted Avg\"\"\"\n",
    "        if data == []:\n",
    "            data.append(round(precision_score(target, pred, average='weighted'), 3))\n",
    "            data.append(round(recall_score(target, pred, average='weighted'), 3))\n",
    "            data.append(round(f1_score(target, pred, average='weighted'), 3))\n",
    "            data.append(round(accuracy_score(target, pred,), 3))\n",
    "        for i in range(len(self.headlines)):\n",
    "            self.dataFrame.loc[row][i] = data[i]   \n",
    "        return self.dataFrame.loc[row]\n",
    "            \n",
    "    def table(self):\n",
    "        return self.dataFrame\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.dataFrame}'\n",
    "\n",
    "delimiter = \"+++++++++++++++++++++++++++++++++++++++++++\"\n",
    "result_table = Table(indexes=[\"Случайный лес с оптимальными параметрами\", \"Метод опорных векторов с оптимальными параметрами\",\n",
    "                             \"Случайный лес с оптимальными параметрами(Glove)\", \"Метод опорных векторов с оптимальными параметрами(Glove)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0b2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dff0a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: \n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f9bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea55358",
   "metadata": {},
   "source": [
    "# 1. Загрузить выборку стихотворений одного из поэтов в соответствии с вариантом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b77bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем поэта\n",
    "poet = 'mayakovskiy' #@param ['mayakovskiy', 'pushkin']\n",
    "\n",
    "path_to_file = f'{poet}.txt'\n",
    "path_to_file = tf.keras.utils.get_file(path_to_file, f'http://uit.mpei.ru/git/main/TDA/raw/branch/master/assets/poems/{path_to_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceeb6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 815675 characters\n"
     ]
    }
   ],
   "source": [
    "# Загружаем текст из файла.\n",
    "# Стихотворения в файле разделены токеном '</s>' - сохраняем в переменную\n",
    "with open(path_to_file,encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "\n",
    "EOS_TOKEN = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db227ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Igorexy\\\\.keras\\\\datasets\\\\mayakovskiy.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a929d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Угрюмый дождь скосил глаза.\n",
      "А за\n",
      "решеткой\n",
      "четкой\n",
      "железной мысли проводов —\n",
      "перина.\n",
      "И на\n",
      "нее\n",
      "встающих звезд\n",
      "легко оперлись ноги.\n",
      "Но ги —\n",
      "бель фонарей,\n",
      "царей\n",
      "в короне газа,\n",
      "для глаза\n",
      "сделала больней\n",
      "враждующий букет бульварных проституток.\n",
      "И жуток\n",
      "шуток\n",
      "клюющий смех —\n",
      "из желтых\n",
      "ядовитых роз\n",
      "возрос\n",
      "зигзагом.\n",
      "За гам\n",
      "и жуть\n",
      "взглянуть\n",
      "отрадно глазу:\n",
      "раба\n",
      "крестов\n",
      "страдающе-спокойно-безразличных,\n",
      "гроба\n",
      "домов\n",
      "публичных\n",
      "восток бросал в одну пылающую вазу.\n",
      "\n",
      "</s>\n",
      "\n",
      "\n",
      "У —\n",
      "лица.\n",
      "Лица\n",
      "у\n",
      "догов\n",
      "годов\n",
      "рез —\n",
      "че.\n",
      "Че\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на текст\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0c65c",
   "metadata": {},
   "source": [
    "# 2. Познакомиться с данными. Проанализировать статистические характеристики исходных данных (среднюю длину стихотворения, среднюю длину строки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9537719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_line_len(poem):\n",
    "    lines = [len(line.strip()) for line in poem.split('\\n') if len(line.strip())>0]\n",
    "    return sum(lines)/len(lines)\n",
    "\n",
    "\n",
    "def describe_poems(text,return_df = False):\n",
    "    poems_list = [poem.strip() for poem in text.split(EOS_TOKEN) if len(poem.strip())>0]\n",
    "    df = pd.DataFrame(data=poems_list,columns=['poem'])\n",
    "    df['len'] = df.poem.map(len)\n",
    "    df['lines'] = df.poem.str.count('\\n')\n",
    "    df['mean_line_len'] = df.poem.map(mean_line_len)\n",
    "    if return_df:\n",
    "        return df\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb991ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem</th>\n",
       "      <th>len</th>\n",
       "      <th>lines</th>\n",
       "      <th>mean_line_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Угрюмый дождь скосил глаза.\\nА за\\nрешеткой\\nч...</td>\n",
       "      <td>449</td>\n",
       "      <td>34</td>\n",
       "      <td>11.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>У —\\nлица.\\nЛица\\nу\\nдогов\\nгодов\\nрез —\\nче.\\...</td>\n",
       "      <td>546</td>\n",
       "      <td>42</td>\n",
       "      <td>11.720930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Какая очаровательная ночь!»\\n«Эта,\\n(указывае...</td>\n",
       "      <td>333</td>\n",
       "      <td>21</td>\n",
       "      <td>14.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Скрипка издергалась, упрашивая,\\nи вдруг разре...</td>\n",
       "      <td>765</td>\n",
       "      <td>45</td>\n",
       "      <td>15.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Войне ли думать:\\n«Некрасиво в шраме»?\\nЕй ли ...</td>\n",
       "      <td>901</td>\n",
       "      <td>69</td>\n",
       "      <td>14.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Зеленые листики —\\nи нет зимы.\\nИдем\\nраздолье...</td>\n",
       "      <td>279</td>\n",
       "      <td>24</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>У меня растут года,\\nбудет и семнадцать.\\nГде ...</td>\n",
       "      <td>3656</td>\n",
       "      <td>280</td>\n",
       "      <td>12.836502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>За море синеволное,\\nза сто земель\\nи вод\\nраз...</td>\n",
       "      <td>666</td>\n",
       "      <td>49</td>\n",
       "      <td>12.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Уважаемые\\nтоварищи потомки!\\nРоясь\\nв сегодня...</td>\n",
       "      <td>3681</td>\n",
       "      <td>243</td>\n",
       "      <td>14.090164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>Открывай страницу-дверь\\nв книжке\\nсамый разны...</td>\n",
       "      <td>1235</td>\n",
       "      <td>102</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>743 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  poem   len  lines  \\\n",
       "0    Угрюмый дождь скосил глаза.\\nА за\\nрешеткой\\nч...   449     34   \n",
       "1    У —\\nлица.\\nЛица\\nу\\nдогов\\nгодов\\nрез —\\nче.\\...   546     42   \n",
       "2    «Какая очаровательная ночь!»\\n«Эта,\\n(указывае...   333     21   \n",
       "3    Скрипка издергалась, упрашивая,\\nи вдруг разре...   765     45   \n",
       "4    Войне ли думать:\\n«Некрасиво в шраме»?\\nЕй ли ...   901     69   \n",
       "..                                                 ...   ...    ...   \n",
       "738  Зеленые листики —\\nи нет зимы.\\nИдем\\nраздолье...   279     24   \n",
       "739  У меня растут года,\\nбудет и семнадцать.\\nГде ...  3656    280   \n",
       "740  За море синеволное,\\nза сто земель\\nи вод\\nраз...   666     49   \n",
       "741  Уважаемые\\nтоварищи потомки!\\nРоясь\\nв сегодня...  3681    243   \n",
       "742  Открывай страницу-дверь\\nв книжке\\nсамый разны...  1235    102   \n",
       "\n",
       "     mean_line_len  \n",
       "0        11.857143  \n",
       "1        11.720930  \n",
       "2        14.181818  \n",
       "3        15.652174  \n",
       "4        14.344828  \n",
       "..             ...  \n",
       "738      10.200000  \n",
       "739      12.836502  \n",
       "740      12.340000  \n",
       "741      14.090164  \n",
       "742      11.000000  \n",
       "\n",
       "[743 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_df = describe_poems(text,return_df = True)\n",
    "poem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63e0dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>lines</th>\n",
       "      <th>mean_line_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>743.000000</td>\n",
       "      <td>743.000000</td>\n",
       "      <td>743.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1088.702557</td>\n",
       "      <td>81.532974</td>\n",
       "      <td>12.900151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>623.199169</td>\n",
       "      <td>49.312455</td>\n",
       "      <td>2.385287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>632.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>11.135068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>952.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>12.339286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1345.500000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>14.682460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4172.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>18.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               len       lines  mean_line_len\n",
       "count   743.000000  743.000000     743.000000\n",
       "mean   1088.702557   81.532974      12.900151\n",
       "std     623.199169   49.312455       2.385287\n",
       "min     203.000000   19.000000       7.636364\n",
       "25%     632.000000   44.000000      11.135068\n",
       "50%     952.000000   72.000000      12.339286\n",
       "75%    1345.500000  106.000000      14.682460\n",
       "max    4172.000000  287.000000      18.454545"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc3dac",
   "metadata": {},
   "source": [
    "# 3. Подготовить выборку для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3875a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poems, test_poems = train_test_split(poem_df.poem.to_list(),test_size = 0.1,random_state = RANDOM_STATE)\n",
    "train_poems, val_poems = train_test_split(train_poems,test_size = 0.1,random_state = RANDOM_STATE)\n",
    "\n",
    "train_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(train_poems)\n",
    "val_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(val_poems)\n",
    "test_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(test_poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cabec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 unique characters\n",
      "['\\n', ' ', '!', '\"', '%', '&', '(', ')', ',', '-', '.', '/', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'J', 'K', 'M', 'N', 'O', 'P', 'R', 'S', 'U', 'V', 'X', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'z', '\\xa0', '«', '»', 'à', 'ç', 'è', 'ö', 'ü', '̀', '́', '·', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', '–', '—', '’', '…', '№', '</s>']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))+[EOS_TOKEN]\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69556508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для подачи на вход нейронной сети необходимо закодировать текст в виде числовой последовательности.\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().decode('utf-8')\n",
    "    \n",
    "def ids_from_text(text):\n",
    "    return ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87e1cea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У Петровой\n",
      "у Надежды\n",
      "tf.Tensor(\n",
      "[ 90   2  86 107 120 118 116 104 116 111   1 121   2  84 102 106 107 108\n",
      " 106 129], shape=(20,), dtype=int64)\n",
      "У Петровой\n",
      "у Надежды\n"
     ]
    }
   ],
   "source": [
    "# пример кодирования\n",
    "ids = ids_from_text(train_poems[:20])\n",
    "res_text = text_from_ids(ids)\n",
    "print(train_poems[:20],ids,res_text,sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0e04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодируем данные и преобразуем их в Датасеты\n",
    "train_ids = ids_from_text(train_poems)\n",
    "val_ids = ids_from_text(val_poems)\n",
    "test_ids = ids_from_text(test_poems)\n",
    "\n",
    "train_ids_dataset = tf.data.Dataset.from_tensor_slices(train_ids)\n",
    "val_ids_dataset = tf.data.Dataset.from_tensor_slices(val_ids)\n",
    "test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "304367a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Весь текст разбивается на последовательности длины seq_length. По этим последовательностям будет предсказываться следующий символ.\n",
    "# Попробовать разные длины - среднюю длину строки, среднюю длину стиха\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(train_ids_dataset)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5fd765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У Петровой\n",
      "у Надежды\n",
      "не имеется одежды.\n",
      "Чтоб купить\n",
      "(пришли деньки!),\n",
      "не имеется деньги́.\n",
      "Ей\n",
      "в расцве\n"
     ]
    }
   ],
   "source": [
    "train_sequences = train_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "val_sequences = val_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "test_sequences = test_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in train_sequences.take(1):\n",
    "  print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64dd9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датасет с input и target строками\n",
    "# target сдвинута относительно input на один символ.\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defce5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_sequences.map(split_input_target)\n",
    "val_dataset = val_sequences.map(split_input_target)\n",
    "test_dataset = test_sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d2e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : В смокинг вштопорен,\n",
      "побрит что надо.\n",
      "По гранд\n",
      "по опере\n",
      "гуляю грандом.\n",
      "Смотрю\n",
      "в антракте —\n",
      "красавка \n",
      "Target:  смокинг вштопорен,\n",
      "побрит что надо.\n",
      "По гранд\n",
      "по опере\n",
      "гуляю грандом.\n",
      "Смотрю\n",
      "в антракте —\n",
      "красавка н\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in val_dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example))\n",
    "    print(\"Target:\", text_from_ids(target_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5e96844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    dataset = (\n",
    "        dataset\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    return dataset \n",
    "\n",
    "train_dataset = prepare_dataset(train_dataset)\n",
    "val_dataset = prepare_dataset(val_dataset)\n",
    "test_dataset = prepare_dataset(test_dataset)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c868f",
   "metadata": {},
   "source": [
    "# 4. Построить нейронную сеть. Тип ячейки RNN выбрать в соответствии с вариантом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85763dd",
   "metadata": {},
   "source": [
    "Модель состоит из трех слоев\n",
    "\n",
    "* `tf.keras.layers.Embedding:` Входной слой. Кодирует каждый идентификатор символа в вектор размерностью embedding_dim;\n",
    "* `tf.keras.layers.SimpleRNN`: Рекуррентный слой на ячейках SimpleRNN в количестве `units=rnn_units` **(Здесь нужно указать тип ячеек в соответствии с вариантом)**\n",
    "* `tf.keras.layers.Dense:` Выходной полносвязный слой размерностью vocab_size, в который выводится вероятность каждого символа в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17ec58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Длина словаря символов\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# размерность Embedding'а\n",
    "embedding_dim = 256 #@param{type:\"number\"}\n",
    "\n",
    "# Параметры RNN-слоя\n",
    "rnn_units = 300 #@param {type:\"number\"}\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5fc9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.simple = tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                                   dropout = dropout_p,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    \n",
    "    if states is None:\n",
    "        states = self.simple.get_initial_state(x)\n",
    "\n",
    "    x, *states = self.simple(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c59bc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c3a2c",
   "metadata": {},
   "source": [
    "### Проверка необученой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "add4114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 141) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на один батч из датасета\n",
    "for input_example_batch, target_example_batch in train_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6a54969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(141,), dtype=float32, numpy=\n",
       "array([ 0.04580479,  0.02026156, -0.02932041,  0.01080876,  0.03008067,\n",
       "       -0.01397174, -0.04088385,  0.00980235, -0.01136458, -0.03861335,\n",
       "        0.03159821, -0.03741074,  0.00188887, -0.03991881, -0.05089642,\n",
       "       -0.00251933,  0.04797433,  0.00983226,  0.03675636, -0.00229334,\n",
       "        0.01785504,  0.02266506, -0.04054524, -0.00098924,  0.01531633,\n",
       "        0.04144016, -0.01244012,  0.00970926,  0.02226621,  0.05057633,\n",
       "       -0.02856871, -0.03057659, -0.05018119,  0.01293335, -0.01567086,\n",
       "        0.02911002,  0.02528487,  0.01414949,  0.00015195, -0.06126463,\n",
       "       -0.02632061,  0.02134457,  0.03885552,  0.01414965, -0.00763898,\n",
       "       -0.00962067,  0.01888105,  0.01272053,  0.00853734,  0.01946685,\n",
       "        0.02413196,  0.01342708,  0.03141851,  0.02057206, -0.02138245,\n",
       "       -0.02276708,  0.01623563,  0.0356742 ,  0.04675717, -0.00521255,\n",
       "        0.00065888, -0.02123081, -0.01535884, -0.01793941,  0.0008652 ,\n",
       "        0.01904535, -0.0017634 , -0.01124689,  0.0408749 , -0.01734538,\n",
       "        0.04668616,  0.00356321,  0.01731787, -0.04720968,  0.00557755,\n",
       "       -0.05262648, -0.00595635,  0.00698223, -0.03015682, -0.01928898,\n",
       "       -0.06592335, -0.03422232,  0.04133305, -0.02266137, -0.02372639,\n",
       "       -0.02542889,  0.01523644,  0.03334732,  0.00251937, -0.06012999,\n",
       "       -0.00193446, -0.05434713,  0.0728987 , -0.00351683,  0.00129867,\n",
       "        0.03129939, -0.05121452, -0.00958773, -0.00298541, -0.05436659,\n",
       "       -0.01143583,  0.03099488,  0.01377732,  0.01455922, -0.00973296,\n",
       "       -0.01368571, -0.03331513,  0.05720209, -0.043284  ,  0.00344114,\n",
       "       -0.02159373, -0.0017824 ,  0.00601393,  0.01710441, -0.02227978,\n",
       "       -0.04933154,  0.0454798 , -0.02775414, -0.01700699,  0.03626461,\n",
       "       -0.0068559 , -0.01314778,  0.02090901, -0.02314783,  0.0373836 ,\n",
       "        0.00563116, -0.05625489,  0.00072216,  0.02506263,  0.00706609,\n",
       "        0.02618907,  0.03167513, -0.06317972, -0.01686868,  0.03182201,\n",
       "        0.01944482,  0.00367575,  0.03189451,  0.05143965, -0.01240999,\n",
       "        0.04422163], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "792d4a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([113, 113,  67,  22,   4, 133,  68,   2,  16, 138,   4,  49,  99,\n",
       "        86,  49,  17, 138,  10, 136,  68,  28,  27, 132,  41,  84, 102,\n",
       "        54,  72,  66,  30,  98,  58,  94,  34,  86,  98, 105, 140,  95,\n",
       "       114,  24, 124,  20,  73, 134,  95,  37,  85,  46,  91, 133, 112,\n",
       "        22,  93, 138, 106, 136,  70,  44, 133,  47, 139,  84, 124, 116,\n",
       "       108, 130,  80,  78,  50,  32,  10,  61,  49,  83,  58,  49,   9,\n",
       "        86, 124,  46,  38,  53, 135, 114,  97,   9,  49,  86, 137,  29,\n",
       "        45, 138,  70, 101, 105, 131, 127,  23,  26], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5374977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " трочки,\n",
      "краснодеревщики\n",
      "не слали мебель на́ дом.\n",
      "И кроме\n",
      "свежевымытой сорочки,\n",
      "скажу по совести,\n",
      "мне\n",
      "\n",
      "Next Char Predictions:\n",
      " ллüE\"я̀ >…\"mЭПm?…-—̀MKюdНаsБöOЬwЧUПЬг</s>ШмHцCВёШZОiФякEЦ…д—·gяj№НцожьЙЗnR-«mМwm,Пцiar–мЫ,mП’Nh…·ЯгэщGJ\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327feb7",
   "metadata": {},
   "source": [
    "# 5. Обучить нейронную сеть на разных количествах эпох (5, 15, 30, 50, 70) при зафиксированных параметрах embedding_dim = 256, rnn_units = 300, T = 0.3 и сравнить результаты генерации (тексты), перплексию и статистические характеристики сгенерированных текстов. Выбрать оптимальное количество эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ecdbf",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf95e2f",
   "metadata": {},
   "source": [
    "Можно представить задачу как задачу классификации - по предыдущему состоянию RNN и входу в данный момент времени предсказать класс (очередной символ).\n",
    "В этом случае работает стандартная функция потерь tf.keras.losses.sparse_categorical_crossentropy- кроссэнтропия, которая равна минус логарифму предсказанной вероятности для верного класса.\n",
    "\n",
    "Поскольку модель возвращает логиты, вам необходимо установить флаг from_logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d30f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae490229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 141)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.9636793, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d129a",
   "metadata": {},
   "source": [
    "Необученная модель не может делать адекватные предсказания. Ее перплексия («коэффициент неопределённости») приблизительно равна размеру словаря. Это говорит о полной неопределенности модели при генерации текста.\n",
    "\n",
    "Перплексия = exp(кроссэнтропия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6101f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  143.11942\n"
     ]
    }
   ],
   "source": [
    "print('perplexity: ',np.exp(example_batch_mean_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b91bc9",
   "metadata": {},
   "source": [
    "Настраиваем обучение, используя метод tf.keras.Model.compile. Используйте tf.keras.optimizers.Adam с аргументами по умолчанию и функцией потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6985901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2623384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  36096     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      multiple                  167100    \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  42441     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,637\n",
      "Trainable params: 245,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6414985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем tf.keras.callbacks.ModelCheckpoint, чтобы убедиться, что контрольные точки сохраняются во время обучения:\n",
    "# Directory where the checkpoints will be saved\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    # Если папки нет, создаем ее\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor=\"val_loss\",\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf14d14",
   "metadata": {},
   "source": [
    "# Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef745614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    \n",
    "  # Этот фрагмент целиком написан с использованием Tensorflow, поэтому его можно выполнять \n",
    "  # не с помощью интерпретатора языка Python, а через граф операций. Это будет значительно быстрее.  \n",
    "  # Для этого воспользуемся декоратором  @tf.function   \n",
    "  @tf.function   \n",
    "  def generate_one_step(self, inputs, states=None,temperature=1.0):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc2403a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_EPOCHS = Table(headlines=['eval loss', 'perplexity', 'result_text', 'Run time'], \n",
    "                     indexes=['EPOCHS 5', 'EPOCHS 15', 'EPOCHS 30', 'EPOCHS 50', 'EPOCHS 70'], weighted=False)\n",
    "describe_poems_dict_EPOCHS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58572c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Длина словаря символов\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# размерность Embedding'а\n",
    "embedding_dim = 256 #@param{type:\"number\"}\n",
    "\n",
    "# Параметры RNN-слоя\n",
    "rnn_units = 300 #@param {type:\"number\"}\n",
    "dropout_p = 0.5\n",
    "\n",
    "T = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "N = 1000\n",
    "\n",
    "EPOCHS = [5, 15, 30, 50, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fffc903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "102/102 [==============================] - 12s 105ms/step - loss: 3.2137 - val_loss: 2.7415\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 2.6613 - val_loss: 2.5758\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 10s 95ms/step - loss: 2.5582 - val_loss: 2.4985\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 2.4970 - val_loss: 2.4497\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.4529 - val_loss: 2.4113\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 2.4053\n",
      "eval loss: 2.405268430709839\n",
      "perplexity 11.08140447225509\n",
      "\n",
      "Всем бростая.\n",
      "В странной в долонки.\n",
      "\n",
      "</s>\n",
      "\n",
      "Верет сталить на поровать страте —\n",
      "и сторований востанной\n",
      "стротит\n",
      "стором породите.\n",
      "Пот бороните на поровите,\n",
      "как породить волонной лизит\n",
      "в отонь,\n",
      "половой поредеть —\n",
      "молонить,\n",
      "стоть\n",
      "в стопуть в простома —\n",
      "не на постару.\n",
      "В сторот в востовать —\n",
      "не строть не водет\n",
      "посторами серется\n",
      "в разной поредита.\n",
      "В сторать в кольше,\n",
      "серет породное старища.\n",
      "Разовеникой морок.\n",
      "Собот каренный\n",
      "как не простовать не настью —\n",
      "велит\n",
      "и только линной вать.\n",
      "Польцет\n",
      "не востовалинь,\n",
      "маль —\n",
      "порето в сталить,\n",
      "вот на восталь —\n",
      "поленном сторок.\n",
      "Порешенный польценный стором.\n",
      "Польше —\n",
      "крусто в есторной —\n",
      "столькой сверет с городной простов.\n",
      "Будет в породной —\n",
      "полет —\n",
      "толь —\n",
      "и в сомо\n",
      "сольце и поредной —\n",
      "столой поредной породной\n",
      "в половеть волином и простов,\n",
      "не сталини.\n",
      "В скорется в сорости,\n",
      "сторать на всторовать —\n",
      "в с мерет\n",
      "с беле в просте —\n",
      "не подолеть\n",
      "в коретной —\n",
      "на поредит\n",
      "половой —\n",
      "столеника.\n",
      "Полька —\n",
      "сторов стора.\n",
      "Что колонов деленный.\n",
      "В столоний,\n",
      "по востовой каловей.\n",
      "И воле\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.0405616760253906\n",
      "Epoch 1/15\n",
      "102/102 [==============================] - 9s 84ms/step - loss: 2.4165 - val_loss: 2.3783\n",
      "Epoch 2/15\n",
      "102/102 [==============================] - 9s 84ms/step - loss: 2.3852 - val_loss: 2.3536\n",
      "Epoch 3/15\n",
      "102/102 [==============================] - 9s 84ms/step - loss: 2.3588 - val_loss: 2.3235\n",
      "Epoch 4/15\n",
      "102/102 [==============================] - 9s 84ms/step - loss: 2.3308 - val_loss: 2.2963\n",
      "Epoch 5/15\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.3086 - val_loss: 2.2765\n",
      "Epoch 6/15\n",
      "102/102 [==============================] - 11s 99ms/step - loss: 2.2874 - val_loss: 2.2556\n",
      "Epoch 7/15\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.2681 - val_loss: 2.2338\n",
      "Epoch 8/15\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.2500 - val_loss: 2.2179\n",
      "Epoch 9/15\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.2342 - val_loss: 2.2048\n",
      "Epoch 10/15\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.2201 - val_loss: 2.1904\n",
      "Epoch 11/15\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.2054 - val_loss: 2.1788\n",
      "Epoch 12/15\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.1936 - val_loss: 2.1684\n",
      "Epoch 13/15\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.1820 - val_loss: 2.1568\n",
      "Epoch 14/15\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.1722 - val_loss: 2.1472\n",
      "Epoch 15/15\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.1626 - val_loss: 2.1410\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 2.1442\n",
      "eval loss: 2.1441650390625\n",
      "perplexity 8.534911943289629\n",
      "\n",
      "не света\n",
      "на положенный стой!\n",
      "Хоть на нет на под простаниться —\n",
      "и на костиков сталенный старовится\n",
      "под него не последников бы\n",
      "в буржуем в коронной вод.\n",
      "Вот —\n",
      "самого город под коммунов в старах сказали —\n",
      "теперь\n",
      "колосов в город —\n",
      "на мородит\n",
      "с верста —\n",
      "стройки рабочих восторки,\n",
      "без третьем,\n",
      "под кончино.\n",
      "Под по столоника.\n",
      "Вот поставить не вот —\n",
      "не все под будет\n",
      "рабочий под странном старой —\n",
      "не по старой стоит —\n",
      "и в от старой стой,\n",
      "как было на простой,\n",
      "с под колосов —\n",
      "и стали и за нас\n",
      "на этой старой лица —\n",
      "с только столу —\n",
      "не старите —\n",
      "были полеса.\n",
      "\n",
      "\n",
      "Вот день в под коммунисте.\n",
      "В замера\n",
      "под молодом —\n",
      "не постивает от стал —\n",
      "в под коронно —\n",
      "и старой под под косторой —\n",
      "и старает коммунисто.\n",
      "В старает под коморой.\n",
      "\n",
      "\n",
      "Со когда\n",
      "на страна —\n",
      "не под стени,\n",
      "в коммуни половы —\n",
      "в буржуем старом в старой восторой —\n",
      "в просто на коммуни,\n",
      "в любовь и станет\n",
      "и в нельзя в нас\n",
      "по комсомольцых старами —\n",
      "не на старом страница,\n",
      "не старом трудно\n",
      "и под стали пошля в громах барабочки.\n",
      "В не выстрелись\n",
      "в под старой солнц\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.9057626724243164\n",
      "Epoch 1/30\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.1520 - val_loss: 2.1337\n",
      "Epoch 2/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.1450 - val_loss: 2.1236\n",
      "Epoch 3/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.1358 - val_loss: 2.1179\n",
      "Epoch 4/30\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.1286 - val_loss: 2.1114\n",
      "Epoch 5/30\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.1209 - val_loss: 2.1090\n",
      "Epoch 6/30\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.1145 - val_loss: 2.1005\n",
      "Epoch 7/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.1083 - val_loss: 2.0933\n",
      "Epoch 8/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.1018 - val_loss: 2.0921\n",
      "Epoch 9/30\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.0964 - val_loss: 2.0892\n",
      "Epoch 10/30\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.0897 - val_loss: 2.0853\n",
      "Epoch 11/30\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.0858 - val_loss: 2.0776\n",
      "Epoch 12/30\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.0810 - val_loss: 2.0755\n",
      "Epoch 13/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0749 - val_loss: 2.0707\n",
      "Epoch 14/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0705 - val_loss: 2.0691\n",
      "Epoch 15/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0664 - val_loss: 2.0679\n",
      "Epoch 16/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0621 - val_loss: 2.0609\n",
      "Epoch 17/30\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0581 - val_loss: 2.0592\n",
      "Epoch 18/30\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0538 - val_loss: 2.0545\n",
      "Epoch 19/30\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.0508 - val_loss: 2.0495\n",
      "Epoch 20/30\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.0462 - val_loss: 2.0492\n",
      "Epoch 21/30\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0443 - val_loss: 2.0472\n",
      "Epoch 22/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0397 - val_loss: 2.0473\n",
      "Epoch 23/30\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0358 - val_loss: 2.0409\n",
      "Epoch 24/30\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0336 - val_loss: 2.0392\n",
      "Epoch 25/30\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.0316 - val_loss: 2.0367\n",
      "Epoch 26/30\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 2.0275 - val_loss: 2.0340\n",
      "Epoch 27/30\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0249 - val_loss: 2.0348\n",
      "Epoch 28/30\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0220 - val_loss: 2.0339\n",
      "Epoch 29/30\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.0196 - val_loss: 2.0316\n",
      "Epoch 30/30\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.0172 - val_loss: 2.0306\n",
      "12/12 [==============================] - 1s 37ms/step - loss: 2.0412\n",
      "eval loss: 2.0411767959594727\n",
      "perplexity 7.699664803964806\n",
      "\n",
      "одно сказать в гром деньгу дома\n",
      "не выставил\n",
      "в столовый простой.\n",
      "На коротки\n",
      "и в коммунисты в перед простой.\n",
      "Не под красный коммунизм\n",
      "в сердце\n",
      "под коммунисты в кровью домой просто —\n",
      "под ногами в коммунистов.\n",
      "На помни —\n",
      "вот тебе\n",
      "и в одной полезовый мальчики.\n",
      "На верную страна —\n",
      "в громадарий,\n",
      "под коммунизм\n",
      "в коммунизм волосья.\n",
      "И в столет в мире\n",
      "с пороженые\n",
      "рабочий карандарный просто,\n",
      "как в небесной страна.\n",
      "На последней помоще —\n",
      "старает на короткий волос,\n",
      "как в прочей короник,\n",
      "с представить не затих,\n",
      "на полезаре\n",
      "в страна своих крестьянин —\n",
      "в сердце в дальше в столки.\n",
      "\n",
      "</s>\n",
      "\n",
      "Просто\n",
      "в больше революция за красный красной положились в конце\n",
      "в рабочий страна.\n",
      "В коммунизм в каждой коммунисты на красной глаза\n",
      "и до короле в старах —\n",
      "и вот в так под всех под коммунизм\n",
      "не стал и больше,\n",
      "на коммунизм\n",
      "до на страны простой\n",
      "кончем бы\n",
      "в любовью положиться,\n",
      "по дорого —\n",
      "скоро в небо в без комсомольцы.\n",
      "\n",
      "</s>\n",
      "\n",
      "Не будет в крова —\n",
      "в года вырос.\n",
      "На парижной корокой.\n",
      "Не до станет в станет —\n",
      "в темной короле,\n",
      "в поло\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.0619685649871826\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 9s 85ms/step - loss: 2.0135 - val_loss: 2.0276\n",
      "Epoch 2/50\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0129 - val_loss: 2.0265\n",
      "Epoch 3/50\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0091 - val_loss: 2.0248\n",
      "Epoch 4/50\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.0074 - val_loss: 2.0229\n",
      "Epoch 5/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.0057 - val_loss: 2.0255\n",
      "Epoch 6/50\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 2.0046 - val_loss: 2.0218\n",
      "Epoch 7/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 2.0007 - val_loss: 2.0196\n",
      "Epoch 8/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9991 - val_loss: 2.0188\n",
      "Epoch 9/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9973 - val_loss: 2.0211\n",
      "Epoch 10/50\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 1.9957 - val_loss: 2.0178\n",
      "Epoch 11/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9927 - val_loss: 2.0154\n",
      "Epoch 12/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9905 - val_loss: 2.0135\n",
      "Epoch 13/50\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9895 - val_loss: 2.0129\n",
      "Epoch 14/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9881 - val_loss: 2.0103\n",
      "Epoch 15/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9861 - val_loss: 2.0116\n",
      "Epoch 16/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9851 - val_loss: 2.0123\n",
      "Epoch 17/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9828 - val_loss: 2.0074\n",
      "Epoch 18/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9821 - val_loss: 2.0080\n",
      "Epoch 19/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9790 - val_loss: 2.0092\n",
      "Epoch 20/50\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 1.9784 - val_loss: 2.0066\n",
      "Epoch 21/50\n",
      "102/102 [==============================] - 9s 86ms/step - loss: 1.9759 - val_loss: 2.0076\n",
      "Epoch 22/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9745 - val_loss: 2.0032\n",
      "Epoch 23/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9733 - val_loss: 2.0050\n",
      "Epoch 24/50\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 1.9721 - val_loss: 2.0042\n",
      "Epoch 25/50\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9711 - val_loss: 2.0012\n",
      "Epoch 26/50\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9690 - val_loss: 2.0011\n",
      "Epoch 27/50\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9682 - val_loss: 1.9999\n",
      "Epoch 28/50\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9670 - val_loss: 2.0001\n",
      "Epoch 29/50\n",
      "102/102 [==============================] - 10s 94ms/step - loss: 1.9661 - val_loss: 1.9990\n",
      "Epoch 30/50\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9642 - val_loss: 2.0002\n",
      "Epoch 31/50\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9623 - val_loss: 2.0012\n",
      "Epoch 32/50\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9612 - val_loss: 1.9958\n",
      "Epoch 33/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9608 - val_loss: 1.9962\n",
      "Epoch 34/50\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9593 - val_loss: 1.9974\n",
      "Epoch 35/50\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9569 - val_loss: 1.9949\n",
      "Epoch 36/50\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 1.9569 - val_loss: 1.9928\n",
      "Epoch 37/50\n",
      "102/102 [==============================] - 10s 94ms/step - loss: 1.9558 - val_loss: 1.9994\n",
      "Epoch 38/50\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9551 - val_loss: 1.9945\n",
      "Epoch 39/50\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9547 - val_loss: 1.9969\n",
      "Epoch 40/50\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9530 - val_loss: 1.9950\n",
      "Epoch 41/50\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 1.9516 - val_loss: 1.9927\n",
      "Epoch 42/50\n",
      "102/102 [==============================] - 10s 94ms/step - loss: 1.9515 - val_loss: 1.9939\n",
      "Epoch 43/50\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9482 - val_loss: 1.9922\n",
      "Epoch 44/50\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9484 - val_loss: 1.9957\n",
      "Epoch 45/50\n",
      "102/102 [==============================] - 9s 87ms/step - loss: 1.9471 - val_loss: 1.9928\n",
      "Epoch 46/50\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9466 - val_loss: 1.9926\n",
      "Epoch 47/50\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9464 - val_loss: 1.9882\n",
      "Epoch 48/50\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9450 - val_loss: 1.9872\n",
      "Epoch 49/50\n",
      "102/102 [==============================] - 10s 97ms/step - loss: 1.9438 - val_loss: 1.9944\n",
      "Epoch 50/50\n",
      "102/102 [==============================] - 11s 107ms/step - loss: 1.9443 - val_loss: 1.9892\n",
      "12/12 [==============================] - 1s 39ms/step - loss: 2.0031\n",
      "eval loss: 2.003133535385132\n",
      "perplexity 7.412246282334065\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Вот —\n",
      "и простоло\n",
      "и в разумеется —\n",
      "не стоят\n",
      "и в просто\n",
      "на коммунизма\n",
      "и в старание —\n",
      "не с перед нам не было\n",
      "и не слово —\n",
      "не под нем —\n",
      "не слово\n",
      "в просторовой колоний.\n",
      "Не было бы\n",
      "выставь на красной,\n",
      "не слово\n",
      "примерами в простой.\n",
      "Не смех —\n",
      "не выполняет старом.\n",
      "Не старает коммунисты.\n",
      "Бедный\n",
      "в каждой банда бы —\n",
      "не своим столотой разумение красного восторга.\n",
      "На под страна —\n",
      "не в каждой раз —\n",
      "будет староваться —\n",
      "только страна\n",
      "на всех красной странах\n",
      "какой-то\n",
      "и в старом слова —\n",
      "и под коммунизм на конец,\n",
      "в против собственной солнце —\n",
      "как на страна —\n",
      "в стальный страна.\n",
      "В странах\n",
      "на колебанского верной лошадь.\n",
      "В конца —\n",
      "как бы\n",
      "не стальные получиться —\n",
      "и старая не собственно —\n",
      "сердце —\n",
      "не в признанье\n",
      "стала как с наших —\n",
      "все не стали —\n",
      "не под воздух\n",
      "в грязь в дома —\n",
      "в старых разве?\n",
      "Не версты от старом —\n",
      "сердце\n",
      "в старание\n",
      "на время\n",
      "не стал и просторов.\n",
      "На нас\n",
      "не строительный колоний.\n",
      "В сердце\n",
      "простой кровать.\n",
      "Не свистит\n",
      "под коммунисты\n",
      "и простор,\n",
      "на старом морде —\n",
      "стоит в небо\n",
      "с небо\n",
      "в семейной к\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.2844164371490479\n",
      "Epoch 1/70\n",
      "102/102 [==============================] - 11s 105ms/step - loss: 1.9423 - val_loss: 1.9893\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 11s 103ms/step - loss: 1.9414 - val_loss: 1.9912\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9400 - val_loss: 1.9877\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9384 - val_loss: 1.9870\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 10s 94ms/step - loss: 1.9391 - val_loss: 1.9869\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 10s 94ms/step - loss: 1.9369 - val_loss: 1.9891\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 10s 95ms/step - loss: 1.9365 - val_loss: 1.9861\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9368 - val_loss: 1.9870\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9353 - val_loss: 1.9855\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 10s 95ms/step - loss: 1.9345 - val_loss: 1.9863\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 11s 101ms/step - loss: 1.9326 - val_loss: 1.9864\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 12s 110ms/step - loss: 1.9328 - val_loss: 1.9856\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 14s 127ms/step - loss: 1.9315 - val_loss: 1.9863\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 1.9317 - val_loss: 1.9877\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 14s 137ms/step - loss: 1.9303 - val_loss: 1.9863\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 11s 98ms/step - loss: 1.9291 - val_loss: 1.9843\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9291 - val_loss: 1.9880\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9288 - val_loss: 1.9856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/70\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9287 - val_loss: 1.9871\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9266 - val_loss: 1.9854\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9258 - val_loss: 1.9843\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9264 - val_loss: 1.9838\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9247 - val_loss: 1.9845\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9243 - val_loss: 1.9833\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9248 - val_loss: 1.9835\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 10s 97ms/step - loss: 1.9228 - val_loss: 1.9833\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 1.9226 - val_loss: 1.9854\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 1.9223 - val_loss: 1.9831\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 1.9217 - val_loss: 1.9847\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 1.9215 - val_loss: 1.9836\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 1.9197 - val_loss: 1.9817\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 11s 102ms/step - loss: 1.9184 - val_loss: 1.9809\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 11s 99ms/step - loss: 1.9189 - val_loss: 1.9801\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9184 - val_loss: 1.9815\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 11s 102ms/step - loss: 1.9182 - val_loss: 1.9830\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 10s 96ms/step - loss: 1.9170 - val_loss: 1.9813\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9168 - val_loss: 1.9819\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9158 - val_loss: 1.9833\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9162 - val_loss: 1.9826\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9150 - val_loss: 1.9780\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9140 - val_loss: 1.9829\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9143 - val_loss: 1.9804\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9125 - val_loss: 1.9820\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9122 - val_loss: 1.9823\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9123 - val_loss: 1.9766\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 11s 103ms/step - loss: 1.9124 - val_loss: 1.9807\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 11s 100ms/step - loss: 1.9116 - val_loss: 1.9800\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 11s 104ms/step - loss: 1.9108 - val_loss: 1.9775\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 11s 99ms/step - loss: 1.9097 - val_loss: 1.9776\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9098 - val_loss: 1.9775\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9087 - val_loss: 1.9770\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9095 - val_loss: 1.9788\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9084 - val_loss: 1.9844\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 10s 92ms/step - loss: 1.9078 - val_loss: 1.9778\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9083 - val_loss: 1.9770\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9059 - val_loss: 1.9796\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 10s 91ms/step - loss: 1.9060 - val_loss: 1.9788\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9055 - val_loss: 1.9790\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 1.9065 - val_loss: 1.9787\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 10s 98ms/step - loss: 1.9055 - val_loss: 1.9802\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 10s 98ms/step - loss: 1.9048 - val_loss: 1.9782\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 11s 106ms/step - loss: 1.9035 - val_loss: 1.9809\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 11s 99ms/step - loss: 1.9044 - val_loss: 1.9812\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 10s 95ms/step - loss: 1.9038 - val_loss: 1.9787\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 1.9042 - val_loss: 1.9771\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 1.9023 - val_loss: 1.9799\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 1.9026 - val_loss: 1.9806\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 1.9028 - val_loss: 1.9791\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 13s 124ms/step - loss: 1.9004 - val_loss: 1.9777\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 1.8998 - val_loss: 1.9775\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 1.9977\n",
      "eval loss: 1.997715950012207\n",
      "perplexity 7.372198384697808\n",
      "\n",
      "и в старом востока\n",
      "и в стране\n",
      "светите\n",
      "не под странах\n",
      "в странной каждой степень.\n",
      "Пока\n",
      "стальные страна.\n",
      "\n",
      "</s>\n",
      "\n",
      "Воздух\n",
      "и скажите,\n",
      "как будто стальные головы,\n",
      "как в водки —\n",
      "в лицо на конец —\n",
      "за конец казалось —\n",
      "и в общей старая на писательно —\n",
      "и в самого запрятая на комсомольцев,\n",
      "на всех делами положились не в коммунисты.\n",
      "\n",
      "</s>\n",
      "\n",
      "Сто старому\n",
      "в страницы машина.\n",
      "Под коммунизма\n",
      "со стала по два дело\n",
      "по воздух\n",
      "в облака —\n",
      "в странах волны,\n",
      "не слово восток\n",
      "и в странах —\n",
      "стало в стенками страна,\n",
      "в странах\n",
      "по страсть со светит,\n",
      "на воздух.\n",
      "\n",
      "\n",
      ". Вот так в столицы воздух,\n",
      "и в облаков ли на коронов он —\n",
      "и в грязь\n",
      "в головой деревне —\n",
      "и то в столетия —\n",
      "под колесками затороженный пись.\n",
      "За небесам молодой на весело.\n",
      "А теперь —\n",
      "до красной детей,\n",
      "под старое старый —\n",
      "не смешков на старенький боков,\n",
      "и в странах всех на странах —\n",
      "от водки в коммунистых\n",
      "ответственный выставь!\n",
      "В старом странах\n",
      "в каждой страна дома,\n",
      "и в старом только и в странах страна.\n",
      "Не просто и самоварный,\n",
      "собственной комплекаться.\n",
      "На страна под к\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.3615853786468506\n"
     ]
    }
   ],
   "source": [
    "for i in EPOCHS:\n",
    "    history = model.fit(train_dataset, validation_data = val_dataset, epochs=i, callbacks=[checkpoint_callback])\n",
    "    eval_loss = model.evaluate(test_dataset)\n",
    "    perplexity = np.exp(eval_loss)\n",
    "    print('eval loss:', eval_loss)\n",
    "    print('perplexity', np.exp(eval_loss))\n",
    "    \n",
    "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "    \n",
    "    start = time.time()\n",
    "    states = None\n",
    "    next_char = tf.constant(['\\n'])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(N):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    end = time.time()\n",
    "\n",
    "    result_text = result[0].numpy().decode('utf-8')\n",
    "    print(result_text)\n",
    "    print('_'*80)\n",
    "    Run_time = end - start\n",
    "    print('\\nRun time:', Run_time)\n",
    "    \n",
    "    result_table_EPOCHS.fill_row(row='EPOCHS '+str(i), data=[eval_loss, perplexity, result_text, Run_time])\n",
    "    describe_poems_dict_EPOCHS.update({i: describe_poems(result_text)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3357e",
   "metadata": {},
   "source": [
    "# 6. Изменяя параметр температуры T проанализировать изменения сгенерированного текста. Выбрать оптимальное значение параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c3de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = [0.1, 0.3, 0.5, 0.7, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "453763ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "T=0.1\n",
      "\n",
      "и стальные день\n",
      "в странах\n",
      "не стальные день\n",
      "в странах волны,\n",
      "в странах старого —\n",
      "в странах крестьянский —\n",
      "и в странах под красной странах\n",
      "в странах страна —\n",
      "не стальные странах\n",
      "в положить в старой —\n",
      "в странах страна —\n",
      "в странах\n",
      "на странах\n",
      "в странах крестьянин,\n",
      "в странах волосами старается —\n",
      "не стальные дома,\n",
      "в странах\n",
      "в странах страна —\n",
      "не стальные дома,\n",
      "на странах\n",
      "не стальные день —\n",
      "не стальные волны,\n",
      "в странах страна —\n",
      "под красной странах —\n",
      "не стальные делаться.\n",
      "Под красной странах —\n",
      "под красной странах\n",
      "в странах страна —\n",
      "не стали по старом странах —\n",
      "не стальные день\n",
      "в странах\n",
      "стальные странах —\n",
      "не стальные странах\n",
      "страна —\n",
      "не стал и старая воздух,\n",
      "на странах старая в старой —\n",
      "стальные странах —\n",
      "в странах\n",
      "в странах страна —\n",
      "в странах казане.\n",
      "\n",
      "</s>\n",
      "\n",
      "В каждой страна —\n",
      "не стали в коммунистых не старая,\n",
      "в странах страна —\n",
      "и в старом странах —\n",
      "в странах страна вода.\n",
      "В каждый странах\n",
      "в странах дело —\n",
      "не стальные день\n",
      "в странах волосами стальные день,\n",
      "на старому\n",
      "в странах старая в коммунизма\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.4270515441894531\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "T=0.3\n",
      "\n",
      "по старому разве —\n",
      "и в мордой страна.\n",
      "Под наших красной верстых разве?\n",
      "Но вы в стране\n",
      "и свете —\n",
      "на компанием по старание,\n",
      "как будто старайся,\n",
      "и в столовый вода.\n",
      "На старая и положить.\n",
      "В небесной коленки.\n",
      "Под странах машины,\n",
      "в себе\n",
      "в три в город —\n",
      "в старом своих добрость —\n",
      "в странах\n",
      "странной морде,\n",
      "и стальные волосами в вас,\n",
      "вот так года не с положил —\n",
      "все делает капиталистых волосами.\n",
      "\n",
      "</s>\n",
      "\n",
      "В каждому\n",
      "и собственный лица,\n",
      "на ней\n",
      "с положить просто —\n",
      "в каждой красной масса,\n",
      "не стали\n",
      "стоит\n",
      "и в собрание —\n",
      "просторовый материя.\n",
      "Смешной колесоном\n",
      "на старая старали от вода —\n",
      "вот то столько и телефонной маршали.\n",
      "По верной слова —\n",
      "и в рабочих барабанься,\n",
      "как собственной странах\n",
      "красной странах разве —\n",
      "отдавали в стенками.\n",
      "В молодой славой странах\n",
      "у нас\n",
      "не сказал —\n",
      "просто стройки,\n",
      "не стали под нами.\n",
      "\n",
      "</s>\n",
      "\n",
      "Вот так в коммунизма слова.\n",
      "\n",
      "</s>\n",
      "\n",
      "В рабочих мост.\n",
      "Но в страницы странности.\n",
      "\n",
      "</s>\n",
      "\n",
      "В вас\n",
      "не слушайте,\n",
      "с каждой страна —\n",
      "семь не в коммунизма\n",
      "на каждого коммунистых\n",
      "положить\n",
      "в страницы\n",
      "и в окна —\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.4065420627593994\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "T=0.5\n",
      "\n",
      "с глаза\n",
      "на столе в петя,\n",
      "на лапа\n",
      "на стенной каждому\n",
      "растита стройкам.\n",
      "С небо молодой свой.\n",
      "Коммунисты телеграфии.\n",
      "Так\n",
      "стоит стальные красной сплетни,\n",
      "как брать во все семья направляя,\n",
      "на нас\n",
      "на обрез,\n",
      "и слова и водки,\n",
      "по трахмирый\n",
      "в губерний направляет рабочих стихи\n",
      "мерев,\n",
      "по себя\n",
      "то собой молодежь!\n",
      "\n",
      "</s>\n",
      "\n",
      "Но нет —\n",
      "и грязной город.\n",
      "Просто на пасть.\n",
      "За дополовий не слова выставь,\n",
      "как будто поэтический и слеза.\n",
      "\n",
      "</s>\n",
      "\n",
      "В бульварений —\n",
      "в какой-то воздушных города\n",
      "на туши и пули сила\n",
      "молоды,\n",
      "во все разве\n",
      "в грязды часто.\n",
      "В места трещиной.\n",
      "Но за окон\n",
      "за говорят примери.\n",
      "Колонный коммунизм.\n",
      "\n",
      "</s>\n",
      "\n",
      "Я стального крестьянский —\n",
      "с половин на слезами ли?\n",
      "Трудом\n",
      "по площади и дням облака.\n",
      "Не мы —\n",
      "сердце пролетарий.\n",
      "В странство\n",
      "с леса\n",
      "по каждой рабочий случайте\n",
      "в каждой миллионами запах\n",
      "да хамали,\n",
      "над тогда не возьмишь,\n",
      "смотрите —\n",
      "собственной китай.\n",
      "Владимиразами бы\n",
      "в столовой славочина.\n",
      "Но во всех трубы я\n",
      "до петров и труд,\n",
      "и в кадется\n",
      "не стих заборами\n",
      "следующий каждого и пятиконых мира —\n",
      "в окошной нет\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.3564417362213135\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "T=0.7\n",
      "\n",
      "и вот каплют на мелочью\n",
      "в первых кошучки,\n",
      "снега мощь пасть —\n",
      "и развозить на бездельного награничное той, не надо —\n",
      "по воздух,\n",
      "до ночь.\n",
      "Где же —\n",
      "от одно,\n",
      "направляют короткой волочит,\n",
      "в покойник на всех\n",
      "сказать\n",
      "звезды издывайте покорном стороны.\n",
      "Позвонят во тьму ухо не видят,\n",
      "штыкомый скасстой,\n",
      "вы их буржуя\n",
      "страна всех что\n",
      "неводит он война.\n",
      "Совсем креплить,\n",
      "как стал,\n",
      "и встает молчановцем рост.\n",
      "\n",
      "</s>\n",
      "\n",
      "Ленин\n",
      "да я — и в стран,\n",
      "то я\n",
      "и в старомом стадах\n",
      "лава\n",
      "в Калектора,\n",
      "пользуем\n",
      "из водиц по вода.\n",
      "Для нас\n",
      "на красной строком —\n",
      "да на советское тень —\n",
      "такой потом —\n",
      "как бы!»\n",
      "Я волокит —\n",
      "и самого думает огонь\n",
      "оборачи\n",
      "лежат\n",
      "не стал бы\n",
      "на себе не утром\n",
      "восторгов чайки.\n",
      "Вот\n",
      "на басстой\n",
      "на пустон С казвонин.\n",
      "Он заводил —\n",
      "выводит\n",
      "и высящим\n",
      "и рядом.\n",
      "И не выжедный казак?\n",
      "Когда\n",
      "сухим на прочих,\n",
      "с мирахам мало бы —\n",
      "из говорит.\n",
      "\n",
      "</s>\n",
      "\n",
      "По всех в лощением.\n",
      "На бога с крайной собакой бараб,\n",
      "и то —\n",
      "грязу не вы!\n",
      "Самолета —\n",
      "в у ней и всех старого —\n",
      "«Атакующим, —\n",
      "прошел\n",
      "с стал мире —\n",
      "без было по мирмою.\n",
      "\n",
      "</s>\n",
      "\n",
      "Взнак\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.4025657176971436\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "T=1.0\n",
      "\n",
      "Кук от весной Ширьсу острия.\n",
      "Женаком дом!\n",
      "И без мяскавись.\n",
      "На чистоток\n",
      "от этой\n",
      "давно,\n",
      "ядотно, из спакой,\n",
      "тот винтив\n",
      "намочащий ниту.\n",
      "Изобремая броненосцет глядь.\n",
      "Воздушной азат,\n",
      "до стоили —\n",
      "шпейцы,\n",
      "а если ты «вода,\n",
      "миного\n",
      "Читают, товарищ тучи,\n",
      "хватить на наши.\n",
      "Любьчая захкти.\n",
      "Как гладко\n",
      "цифров Автомокихов,\n",
      "то\n",
      "зим,\n",
      "честники.\n",
      "Плещить и коток.\n",
      "Англий окирыньте!\n",
      "Мочесь,\n",
      "штана\n",
      "в-Окорок стали\n",
      "в жизнь хороша!\n",
      "Солнца́н,\n",
      "а за страха\n",
      "щипольно водой\n",
      "нельзя ли?\n",
      "Бой!\n",
      "И загонишь канарасное —\n",
      "вас неужесть\n",
      "тяженья стройки,\n",
      "мом помрит,\n",
      "и выгонял —\n",
      "и как дуру,\n",
      "господин пронулась до сосье\n",
      "дворнях сил? —\n",
      "Видин,\n",
      "двинет из-распорта\n",
      "в петлемедцать!\n",
      "Это —\n",
      "тридцатого\n",
      "рукой —\n",
      "за ней коммуниставесь,\n",
      "но сынишь?!\n",
      "Дай\n",
      "и забыло —\n",
      "в Аналый.\n",
      "\n",
      "</s>\n",
      "\n",
      "Ряским под,\n",
      "на это убой,\n",
      "мужеи сказал,\n",
      "размерали в дожит\n",
      "неизвестия\n",
      "вномькуй.\n",
      "Посветем\n",
      "кто тем,\n",
      "не клопока уши,\n",
      "что треднеграфов Фокруп:\n",
      "«Все живое —\n",
      "собака\n",
      "живот его\n",
      "дуплет обовену.\n",
      "Слезами масс.\n",
      "А чужевись, дайте дрянейцухи.\n",
      "Спрашивают,\n",
      "что земное\n",
      "по мечтачей роть,\n",
      "кто ей\n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.8295414447784424\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in Temperature:\n",
    "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "    start = time.time()\n",
    "    states = None\n",
    "    next_char = tf.constant(['\\n'])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(N):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=temp)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    end = time.time()\n",
    "\n",
    "    result_text = result[0].numpy().decode('utf-8')\n",
    "    print('_'*80, f\"T={temp}\", sep='\\n')\n",
    "    print(result_text)\n",
    "    print('_'*80)\n",
    "    Run_time = end - start\n",
    "    print('\\nRun time:', Run_time)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c277e",
   "metadata": {},
   "source": [
    "Сложно визуально определить лучшее значение T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743c686",
   "metadata": {},
   "source": [
    "# 7. Проанализировать зависимость перплексии, скорости обучения, результатов генерации от параметров нейронной сети embedding_dim и rnn_units:\n",
    "* `embedding_dim` = {vocab/4, vocab/2, vocab, vocab * 2, vocab * 4}, где vocab = размер словаря выборки.\n",
    "* `rnn_units` = {10, 100, 300, 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "196cd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_embedding_dim = Table(headlines=['eval loss', 'perplexity', 'result_text', 'Run time'], \n",
    "                     indexes=['embedding_dim 35', 'embedding_dim 70', 'embedding_dim 140', 'embedding_dim 280', 'embedding_dim 560'], weighted=False)\n",
    "describe_poems_dict_embedding_dim = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c749ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Длина словаря символов\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# размерность Embedding'а\n",
    "embedding_dim = [len(vocab)/4, len(vocab)/2, len(vocab), len(vocab) * 2, len(vocab) * 4] #@param{type:\"number\"}\n",
    "embedding_dim = list(map(int, embedding_dim))\n",
    "\n",
    "# Параметры RNN-слоя\n",
    "rnn_units = 300 #@param {type:\"number\"}\n",
    "dropout_p = 0.5\n",
    "\n",
    "T = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f5da3e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 3.4805 - val_loss: 3.1441\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.9587 - val_loss: 2.9159\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 9s 82ms/step - loss: 2.8400 - val_loss: 2.7388\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 9s 83ms/step - loss: 2.7585 - val_loss: 2.6824\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 9s 84ms/step - loss: 2.7530 - val_loss: 2.6629\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 9s 81ms/step - loss: 2.6954 - val_loss: 2.6266\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 10s 90ms/step - loss: 2.6675 - val_loss: 2.6009\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 9s 82ms/step - loss: 2.6448 - val_loss: 2.5808\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.6246 - val_loss: 2.5613\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 9s 79ms/step - loss: 2.6031 - val_loss: 2.5365\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 9s 82ms/step - loss: 2.5860 - val_loss: 2.5204\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.5697 - val_loss: 2.5039\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 9s 88ms/step - loss: 2.5515 - val_loss: 2.4887\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.5372 - val_loss: 2.4716\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.5213 - val_loss: 2.4588\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 8s 76ms/step - loss: 2.5089 - val_loss: 2.4447\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.4958 - val_loss: 2.4365\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.4843 - val_loss: 2.4239\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 8s 78ms/step - loss: 2.4732 - val_loss: 2.4113\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.4619 - val_loss: 2.4006\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 2.4533 - val_loss: 2.3865\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.4428 - val_loss: 2.3830\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.4320 - val_loss: 2.3700\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 8s 68ms/step - loss: 2.4228 - val_loss: 2.3624\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.4157 - val_loss: 2.3527\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.4079 - val_loss: 2.3420\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.3992 - val_loss: 2.3327\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.3931 - val_loss: 2.3262\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 8s 77ms/step - loss: 2.3836 - val_loss: 2.3174\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 9s 75ms/step - loss: 2.3764 - val_loss: 2.3138\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 9s 82ms/step - loss: 2.3689 - val_loss: 2.3021\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.3632 - val_loss: 2.2947\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.3564 - val_loss: 2.2937\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 8s 77ms/step - loss: 2.3510 - val_loss: 2.2845\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.3439 - val_loss: 2.2783\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.3380 - val_loss: 2.2762\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.3327 - val_loss: 2.2685\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.3272 - val_loss: 2.2596\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.3214 - val_loss: 2.2565\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 9s 82ms/step - loss: 2.3168 - val_loss: 2.2528\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.3111 - val_loss: 2.2457\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 9s 85ms/step - loss: 2.3055 - val_loss: 2.2444\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 8s 77ms/step - loss: 2.3016 - val_loss: 2.2352\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 2.2968 - val_loss: 2.2297\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 7s 65ms/step - loss: 2.2924 - val_loss: 2.2320\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 7s 65ms/step - loss: 2.2878 - val_loss: 2.2250\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 7s 65ms/step - loss: 2.2831 - val_loss: 2.2221\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 7s 67ms/step - loss: 2.2800 - val_loss: 2.2151\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.2752 - val_loss: 2.2094\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 7s 64ms/step - loss: 2.2713 - val_loss: 2.2053\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 7s 66ms/step - loss: 2.2671 - val_loss: 2.2059\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 7s 67ms/step - loss: 2.2646 - val_loss: 2.2038\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.2598 - val_loss: 2.1979\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 9s 82ms/step - loss: 2.2564 - val_loss: 2.1925\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 8s 71ms/step - loss: 2.2532 - val_loss: 2.1902\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.2503 - val_loss: 2.1892\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.2466 - val_loss: 2.1806\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 8s 73ms/step - loss: 2.2426 - val_loss: 2.1831\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.2402 - val_loss: 2.1820\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 2.2361 - val_loss: 2.1760\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 10s 87ms/step - loss: 2.2334 - val_loss: 2.1795\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.2325 - val_loss: 2.1742\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 9s 79ms/step - loss: 2.2273 - val_loss: 2.1714\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 2.2260 - val_loss: 2.1667\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 10s 88ms/step - loss: 2.2220 - val_loss: 2.1633\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 10s 89ms/step - loss: 2.2207 - val_loss: 2.1630\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.2197 - val_loss: 2.1602\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 2.2173 - val_loss: 2.1603\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.2127 - val_loss: 2.1563\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 9s 76ms/step - loss: 2.2098 - val_loss: 2.1555\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 2.1559\n",
      "eval loss: 2.155928373336792\n",
      "perplexity 8.635903801404814\n",
      "\n",
      "с дело\n",
      "по стальный под под страна.\n",
      "Постоплать на вострагом,\n",
      "по день\n",
      "на рассказить\n",
      "на воздушенный стиха.\n",
      "В баражный простора.\n",
      "\n",
      "</s>\n",
      "\n",
      "Товарищи на разумение —\n",
      "не смотрит в старья воздуха.\n",
      "С такой столет под страны.\n",
      "Воздух\n",
      "в просто\n",
      "по небо страна видельный следа.\n",
      "По стором по столется —\n",
      "страна —\n",
      "столоните в ворот,\n",
      "на всех странной быть —\n",
      "и в стихой страна.\n",
      "И под волосать с польце.\n",
      "В соберами\n",
      "по старенький половать страница.\n",
      "В от пока\n",
      "страница в полесали,\n",
      "что столет старый страны,\n",
      "с день на воздушки —\n",
      "с старый красных страна.\n",
      "\n",
      "</s>\n",
      "\n",
      "Стольный бы\n",
      "в командерный голоса.\n",
      "\n",
      "</s>\n",
      "\n",
      "Советской страна.\n",
      "Скорит под старанье —\n",
      "под стола страна.\n",
      "В стане попальство.\n",
      "\n",
      "</s>\n",
      "\n",
      "Пока скореть —\n",
      "на рабочий станенной страна.\n",
      "Под красный столеть.\n",
      "Не старом в старах,\n",
      "за стольцы старания.\n",
      "А на старой рабочий воде.\n",
      "Под старом полесали в столет.\n",
      "И в доломенный восторга.\n",
      "По стольце —\n",
      "столет в столеть на стили в столет.\n",
      "Все да кровь\n",
      "по странней\n",
      "не стали на всех страна —\n",
      "не стали будет —\n",
      "не под сторой страна.\n",
      "Не все собетск\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.1477670669555664\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 9s 76ms/step - loss: 3.4365 - val_loss: 2.9904\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 8s 78ms/step - loss: 2.8285 - val_loss: 2.7015\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 9s 83ms/step - loss: 2.6917 - val_loss: 2.6193\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.6293 - val_loss: 2.5686\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 8s 77ms/step - loss: 2.5845 - val_loss: 2.5213\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.5457 - val_loss: 2.4856\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.5164 - val_loss: 2.4597\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 8s 76ms/step - loss: 2.4904 - val_loss: 2.4351\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.4658 - val_loss: 2.4106\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 9s 78ms/step - loss: 2.4465 - val_loss: 2.3883\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 8s 75ms/step - loss: 2.4268 - val_loss: 2.3690\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 8s 71ms/step - loss: 2.4084 - val_loss: 2.3509\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 8s 71ms/step - loss: 2.3920 - val_loss: 2.3345\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.3785 - val_loss: 2.3254\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 9s 81ms/step - loss: 2.3638 - val_loss: 2.3091\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.3510 - val_loss: 2.2936\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.3380 - val_loss: 2.2861\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.3270 - val_loss: 2.2765\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 8s 77ms/step - loss: 2.3162 - val_loss: 2.2617\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 9s 79ms/step - loss: 2.3056 - val_loss: 2.2513\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 8s 73ms/step - loss: 2.2959 - val_loss: 2.2429\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 8s 76ms/step - loss: 2.2861 - val_loss: 2.2361\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 8s 73ms/step - loss: 2.2778 - val_loss: 2.2256\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.2703 - val_loss: 2.2160\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.2621 - val_loss: 2.2100\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 2.2557 - val_loss: 2.2022\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.2471 - val_loss: 2.1943\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.2402 - val_loss: 2.1911\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.2350 - val_loss: 2.1853\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 8s 74ms/step - loss: 2.2280 - val_loss: 2.1782\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 8s 73ms/step - loss: 2.2226 - val_loss: 2.1728\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 8s 73ms/step - loss: 2.2170 - val_loss: 2.1668\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 9s 75ms/step - loss: 2.2116 - val_loss: 2.1668\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 8s 73ms/step - loss: 2.2060 - val_loss: 2.1607\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.2021 - val_loss: 2.1541\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 8s 70ms/step - loss: 2.1972 - val_loss: 2.1532\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 8s 69ms/step - loss: 2.1926 - val_loss: 2.1475\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 8s 72ms/step - loss: 2.1897 - val_loss: 2.1441\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 9s 80ms/step - loss: 2.1837 - val_loss: 2.1446\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 9s 81ms/step - loss: 2.1810 - val_loss: 2.1387\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 10s 87ms/step - loss: 2.1770 - val_loss: 2.1335\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 11s 102ms/step - loss: 2.1729 - val_loss: 2.1301\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 11s 99ms/step - loss: 2.1675 - val_loss: 2.1259\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 11s 92ms/step - loss: 2.1650 - val_loss: 2.1206\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 2.1614 - val_loss: 2.1217\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 11s 94ms/step - loss: 2.1588 - val_loss: 2.1188\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 10s 93ms/step - loss: 2.1554 - val_loss: 2.1174\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 12s 103ms/step - loss: 2.1528 - val_loss: 2.1161\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 12s 106ms/step - loss: 2.1488 - val_loss: 2.1119\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 11s 97ms/step - loss: 2.1465 - val_loss: 2.1075\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 11s 98ms/step - loss: 2.1427 - val_loss: 2.1064\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 11s 99ms/step - loss: 2.1404 - val_loss: 2.1050\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 12s 104ms/step - loss: 2.1385 - val_loss: 2.1060\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 13s 109ms/step - loss: 2.1362 - val_loss: 2.0995\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 12s 103ms/step - loss: 2.1335 - val_loss: 2.0986\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 11s 101ms/step - loss: 2.1310 - val_loss: 2.0948\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 13s 114ms/step - loss: 2.1287 - val_loss: 2.1000\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 14s 124ms/step - loss: 2.1270 - val_loss: 2.0936\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 15s 131ms/step - loss: 2.1241 - val_loss: 2.0903\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 15s 134ms/step - loss: 2.1221 - val_loss: 2.0897\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 14s 121ms/step - loss: 2.1205 - val_loss: 2.0906\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 12s 111ms/step - loss: 2.1176 - val_loss: 2.0880\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 13s 115ms/step - loss: 2.1163 - val_loss: 2.0865\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 11s 104ms/step - loss: 2.1148 - val_loss: 2.0838\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 12s 108ms/step - loss: 2.1124 - val_loss: 2.0843\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 12s 108ms/step - loss: 2.1102 - val_loss: 2.0830\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 12s 104ms/step - loss: 2.1090 - val_loss: 2.0830\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 12s 111ms/step - loss: 2.1079 - val_loss: 2.0836\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 12s 108ms/step - loss: 2.1057 - val_loss: 2.0776\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 12s 105ms/step - loss: 2.1018 - val_loss: 2.0776\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 2.0872\n",
      "eval loss: 2.087181806564331\n",
      "perplexity 8.062162385386124\n",
      "\n",
      "не все в забетать в страно —\n",
      "и столет и в красных марков.\n",
      "На домерить в польский в просто —\n",
      "не под наших в париже,\n",
      "а в старой пока,\n",
      "просто строиться в коммунистов.\n",
      "\n",
      "</s>\n",
      "\n",
      "Не на помни\n",
      "на всех волосов,\n",
      "помните в пользе\n",
      "так чертом\n",
      "под небесной под стопом —\n",
      "не привет\n",
      "в польшим стал и в коммунистик.\n",
      "Полюсительный страна —\n",
      "не страна под сторониться,\n",
      "под дворцов\n",
      "просто\n",
      "в дело деньги,\n",
      "не призумают стройка\n",
      "про столеков без может\n",
      "и в столекина на небесной разбивая не старой —\n",
      "как разных в небесной лежет —\n",
      "не смотрите —\n",
      "не выражет были выстрались —\n",
      "не привет —\n",
      "и в просто не все нам\n",
      "столетья раз, —\n",
      "не выпескайтесь в старом.\n",
      "На стране на польшек.\n",
      "Не хоть по стально\n",
      "в небо —\n",
      "с небесной коммунисти.\n",
      "В деревне\n",
      "в сто в белой стройками.\n",
      "Не приветав может.\n",
      "Не стал просто верница,\n",
      "не следа —\n",
      "не смерть под стих\n",
      "с построим\n",
      "попросились от восторги.\n",
      "На всех каждый под стройки.\n",
      "И в коммунистерный день,\n",
      "в грязно\n",
      "в стенет под в стенет —\n",
      "не выверить\n",
      "и стали —\n",
      "и по мордой воздух\n",
      "в коммунистый,\n",
      "что был за толках,\n",
      "пр\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.6680853366851807\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 16s 135ms/step - loss: 3.3181 - val_loss: 2.8290\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 2.7300 - val_loss: 2.6345\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 2.6186 - val_loss: 2.5536\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 13s 116ms/step - loss: 2.5570 - val_loss: 2.5047\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 14s 131ms/step - loss: 2.5116 - val_loss: 2.4646\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 14s 129ms/step - loss: 2.4768 - val_loss: 2.4299\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 14s 128ms/step - loss: 2.4462 - val_loss: 2.4019\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 15s 137ms/step - loss: 2.4189 - val_loss: 2.3808\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.3953 - val_loss: 2.3532\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.3742 - val_loss: 2.3311\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.3545 - val_loss: 2.3151\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 15s 133ms/step - loss: 2.3366 - val_loss: 2.2926\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 14s 130ms/step - loss: 2.3177 - val_loss: 2.2814\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 14s 125ms/step - loss: 2.3021 - val_loss: 2.2590\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.2874 - val_loss: 2.2457\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.2740 - val_loss: 2.2313\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.2602 - val_loss: 2.2242\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 13s 117ms/step - loss: 2.2498 - val_loss: 2.2107\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 2.2374 - val_loss: 2.1993\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 14s 125ms/step - loss: 2.2263 - val_loss: 2.1897\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 14s 125ms/step - loss: 2.2173 - val_loss: 2.1793\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.2083 - val_loss: 2.1754\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 13s 123ms/step - loss: 2.1994 - val_loss: 2.1644\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.1911 - val_loss: 2.1587\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 14s 127ms/step - loss: 2.1847 - val_loss: 2.1520\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 14s 126ms/step - loss: 2.1769 - val_loss: 2.1464\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 14s 124ms/step - loss: 2.1693 - val_loss: 2.1386\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 14s 127ms/step - loss: 2.1630 - val_loss: 2.1332\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 14s 123ms/step - loss: 2.1568 - val_loss: 2.1282\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 14s 125ms/step - loss: 2.1523 - val_loss: 2.1244\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 14s 125ms/step - loss: 2.1456 - val_loss: 2.1168\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 14s 132ms/step - loss: 2.1406 - val_loss: 2.1128\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 15s 137ms/step - loss: 2.1352 - val_loss: 2.1096\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 12s 113ms/step - loss: 2.1306 - val_loss: 2.1042\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 14s 127ms/step - loss: 2.1246 - val_loss: 2.1018\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 14s 131ms/step - loss: 2.1208 - val_loss: 2.0971\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 16s 149ms/step - loss: 2.1160 - val_loss: 2.0924\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 17s 157ms/step - loss: 2.1121 - val_loss: 2.0901\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 18s 161ms/step - loss: 2.1088 - val_loss: 2.0876\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 17s 157ms/step - loss: 2.1039 - val_loss: 2.0855\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 17s 152ms/step - loss: 2.0998 - val_loss: 2.0832\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 17s 153ms/step - loss: 2.0974 - val_loss: 2.0803\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.0936 - val_loss: 2.0788\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.0896 - val_loss: 2.0719\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 2.0861 - val_loss: 2.0692\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 13s 122ms/step - loss: 2.0829 - val_loss: 2.0685\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 2.0804 - val_loss: 2.0670\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 13s 122ms/step - loss: 2.0765 - val_loss: 2.0668\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 14s 126ms/step - loss: 2.0763 - val_loss: 2.0623\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 15s 133ms/step - loss: 2.0717 - val_loss: 2.0601\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 13s 116ms/step - loss: 2.0693 - val_loss: 2.0588\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 2.0662 - val_loss: 2.0558\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.0639 - val_loss: 2.0540\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.0618 - val_loss: 2.0521\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 13s 122ms/step - loss: 2.0594 - val_loss: 2.0513\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 13s 117ms/step - loss: 2.0557 - val_loss: 2.0496\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.0536 - val_loss: 2.0471\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.0515 - val_loss: 2.0465\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 13s 123ms/step - loss: 2.0490 - val_loss: 2.0435\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 13s 115ms/step - loss: 2.0476 - val_loss: 2.0419\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 12s 113ms/step - loss: 2.0465 - val_loss: 2.0449\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0444 - val_loss: 2.0413\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 2.0411 - val_loss: 2.0432\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 13s 117ms/step - loss: 2.0395 - val_loss: 2.0354\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 13s 122ms/step - loss: 2.0376 - val_loss: 2.0355\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.0362 - val_loss: 2.0359\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 2.0342 - val_loss: 2.0351\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.0327 - val_loss: 2.0318\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 2.0303 - val_loss: 2.0319\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 13s 119ms/step - loss: 2.0288 - val_loss: 2.0320\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 2.0500\n",
      "eval loss: 2.0500378608703613\n",
      "perplexity 7.768195211371039\n",
      "\n",
      "в страна\n",
      "с порох,\n",
      "пока\n",
      "не под ногам\n",
      "в новый красной любовь,\n",
      "с восток и под ногами\n",
      "по странной под нам\n",
      "положили в коммунистом —\n",
      "под ногами положить и в страна.\n",
      "В под наших\n",
      "под нам\n",
      "по светель\n",
      "и стал в положенный под нам\n",
      "придумал моргах,\n",
      "не то что не стальный страны.\n",
      "Не света не слушайте,\n",
      "как и стальный\n",
      "под нам\n",
      "в коммунистом страна.\n",
      "В страшный старой —\n",
      "не до не стальный строительный дома.\n",
      "\n",
      "</s>\n",
      "\n",
      "По стал и старает\n",
      "и слезай —\n",
      "в собственный домами\n",
      "на морду не под ногами.\n",
      "\n",
      "</s>\n",
      "\n",
      "Под страна страна\n",
      "не в страшно стал\n",
      "под ним\n",
      "пролетарий в страна\n",
      "попробуй простой —\n",
      "в носу\n",
      "и стальный глаза.\n",
      "По страна за мордом —\n",
      "в страницы —\n",
      "не под нем\n",
      "положенный волоска.\n",
      "Под страна\n",
      "только под нам\n",
      "просто завода ли\n",
      "с приводи,\n",
      "лапая под нас\n",
      "по страна\n",
      "на пользуй старой в страна.\n",
      "В комнате\n",
      "не по разве —\n",
      "с коммунисты\n",
      "собрать не причисте.\n",
      "Но в полезанный под старах —\n",
      "в только не в страна.\n",
      "Слушайте стихами выставь,\n",
      "не под нам\n",
      "в сердце воздух,\n",
      "в домами на концы.\n",
      "В страна на польской последний волосил\n",
      "и не против,\n",
      "по старый\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.899963140487671\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 17s 142ms/step - loss: 3.1949 - val_loss: 2.7306\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 16s 147ms/step - loss: 2.6455 - val_loss: 2.5636\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 2.5460 - val_loss: 2.4906\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.4862 - val_loss: 2.4399\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 14s 128ms/step - loss: 2.4419 - val_loss: 2.4026\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.4057 - val_loss: 2.3690\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 17s 155ms/step - loss: 2.3741 - val_loss: 2.3413\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 17s 154ms/step - loss: 2.3464 - val_loss: 2.3144\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 17s 159ms/step - loss: 2.3205 - val_loss: 2.2887\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 17s 157ms/step - loss: 2.2966 - val_loss: 2.2638\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.2746 - val_loss: 2.2473\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 16s 145ms/step - loss: 2.2560 - val_loss: 2.2267\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 2.2386 - val_loss: 2.2108\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.2220 - val_loss: 2.1972\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 16s 147ms/step - loss: 2.2074 - val_loss: 2.1834\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 16s 141ms/step - loss: 2.1951 - val_loss: 2.1715\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 15s 141ms/step - loss: 2.1826 - val_loss: 2.1613\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 15s 140ms/step - loss: 2.1705 - val_loss: 2.1502\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 16s 146ms/step - loss: 2.1609 - val_loss: 2.1401\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 16s 145ms/step - loss: 2.1508 - val_loss: 2.1328\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 2.1420 - val_loss: 2.1267\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 2.1334 - val_loss: 2.1196\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 15s 145ms/step - loss: 2.1250 - val_loss: 2.1151\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.1179 - val_loss: 2.1064\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 2.1107 - val_loss: 2.1025\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 2.1050 - val_loss: 2.0947\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 16s 145ms/step - loss: 2.0971 - val_loss: 2.0916\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 15s 138ms/step - loss: 2.0926 - val_loss: 2.0873\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 16s 147ms/step - loss: 2.0881 - val_loss: 2.0848\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 15s 140ms/step - loss: 2.0805 - val_loss: 2.0806\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 15s 144ms/step - loss: 2.0771 - val_loss: 2.0756\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 2.0704 - val_loss: 2.0727\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.0666 - val_loss: 2.0673\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 2.0618 - val_loss: 2.0670\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 16s 149ms/step - loss: 2.0569 - val_loss: 2.0581\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.0539 - val_loss: 2.0582\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.0508 - val_loss: 2.0589\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.0471 - val_loss: 2.0544\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 15s 140ms/step - loss: 2.0437 - val_loss: 2.0519\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 16s 143ms/step - loss: 2.0394 - val_loss: 2.0480\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 15s 140ms/step - loss: 2.0357 - val_loss: 2.0492\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 15s 138ms/step - loss: 2.0330 - val_loss: 2.0458\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 2.0296 - val_loss: 2.0427\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 15s 140ms/step - loss: 2.0261 - val_loss: 2.0398\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 2.0243 - val_loss: 2.0398\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 15s 143ms/step - loss: 2.0205 - val_loss: 2.0370\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 15s 138ms/step - loss: 2.0159 - val_loss: 2.0302\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 2.0142 - val_loss: 2.0323\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 15s 141ms/step - loss: 2.0141 - val_loss: 2.0313\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 14s 134ms/step - loss: 2.0112 - val_loss: 2.0321\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 2.0084 - val_loss: 2.0292\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 16s 150ms/step - loss: 2.0042 - val_loss: 2.0256\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 15s 144ms/step - loss: 2.0029 - val_loss: 2.0252\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 15s 145ms/step - loss: 2.0010 - val_loss: 2.0247\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 1.9989 - val_loss: 2.0259\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 15s 140ms/step - loss: 1.9964 - val_loss: 2.0220\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 15s 138ms/step - loss: 1.9948 - val_loss: 2.0205\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 15s 141ms/step - loss: 1.9934 - val_loss: 2.0182\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 16s 147ms/step - loss: 1.9907 - val_loss: 2.0173\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 16s 146ms/step - loss: 1.9885 - val_loss: 2.0145\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 1.9877 - val_loss: 2.0164\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 15s 136ms/step - loss: 1.9843 - val_loss: 2.0123\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 14s 135ms/step - loss: 1.9848 - val_loss: 2.0150\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 15s 138ms/step - loss: 1.9820 - val_loss: 2.0144\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 14s 135ms/step - loss: 1.9810 - val_loss: 2.0097\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 14s 129ms/step - loss: 1.9788 - val_loss: 2.0125\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 15s 137ms/step - loss: 1.9770 - val_loss: 2.0130\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 14s 132ms/step - loss: 1.9756 - val_loss: 2.0106\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 14s 134ms/step - loss: 1.9740 - val_loss: 2.0110\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 15s 139ms/step - loss: 1.9722 - val_loss: 2.0065\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 2.0240\n",
      "eval loss: 2.024001359939575\n",
      "perplexity 7.568548913247507\n",
      "\n",
      "поставить на всегда —\n",
      "в под вас\n",
      "не в стран\n",
      "в странной после\n",
      "под наших крестьянин —\n",
      "и по ленинцы\n",
      "и в страна —\n",
      "по красных темното\n",
      "по воротал\n",
      "и не выпребят,\n",
      "станет с небесном стали —\n",
      "в свои политики.\n",
      "Под просто не стали в стране —\n",
      "стальные столетой,\n",
      "что в воздуха в под подымает в компания,\n",
      "в странный положить на нашим в столицы!\n",
      "Стальные столетном рукой строчка\n",
      "и вам\n",
      "на полители в старом —\n",
      "с под становится\n",
      "с партийный стали в получить в короне.\n",
      "И в каждого стройка —\n",
      "своим строительной водки,\n",
      "в страна крестьянин на слова —\n",
      "по дома в стране.\n",
      "По случай конец,\n",
      "как будто поэта\n",
      "в под стали в работу —\n",
      "в под становится в карточку в под стал,\n",
      "в стальной под комсомольцев.\n",
      "Скажет —\n",
      "стоит от большевистик —\n",
      "в страна в мордой волосится.\n",
      "В страна —\n",
      "в спину и столетно,\n",
      "стали горящие в половина,\n",
      "в под красной красных в полезет.\n",
      "\n",
      "</s>\n",
      "\n",
      "В стране просто —\n",
      "по страна —\n",
      "старовать\n",
      "в странами в польше строить вытирели.\n",
      "На всех собственный роз.\n",
      "Под страна —\n",
      "стали с наших под нам\n",
      "под под ними строчки —\n",
      "и в небесных\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.5441193580627441\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 28s 240ms/step - loss: 3.0948 - val_loss: 2.6477\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 25s 235ms/step - loss: 2.5825 - val_loss: 2.5113\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 25s 237ms/step - loss: 2.4884 - val_loss: 2.4447\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 26s 245ms/step - loss: 2.4303 - val_loss: 2.3946\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 24s 229ms/step - loss: 2.3845 - val_loss: 2.3514\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 26s 248ms/step - loss: 2.3474 - val_loss: 2.3179\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 24s 228ms/step - loss: 2.3122 - val_loss: 2.2842\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 24s 227ms/step - loss: 2.2833 - val_loss: 2.2583\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 24s 228ms/step - loss: 2.2553 - val_loss: 2.2318\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 26s 242ms/step - loss: 2.2314 - val_loss: 2.2131\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 25s 232ms/step - loss: 2.2112 - val_loss: 2.1977\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 25s 235ms/step - loss: 2.1924 - val_loss: 2.1797\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 19s 178ms/step - loss: 2.1754 - val_loss: 2.1674\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 18s 176ms/step - loss: 2.1607 - val_loss: 2.1493\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 18s 168ms/step - loss: 2.1469 - val_loss: 2.1446\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 17s 163ms/step - loss: 2.1332 - val_loss: 2.1306\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 19s 183ms/step - loss: 2.1219 - val_loss: 2.1207\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 17s 166ms/step - loss: 2.1117 - val_loss: 2.1124\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 17s 163ms/step - loss: 2.1017 - val_loss: 2.1028\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 17s 161ms/step - loss: 2.0916 - val_loss: 2.0960\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 18s 173ms/step - loss: 2.0847 - val_loss: 2.0924\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 19s 176ms/step - loss: 2.0768 - val_loss: 2.0834\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 18s 168ms/step - loss: 2.0688 - val_loss: 2.0791\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 20s 185ms/step - loss: 2.0623 - val_loss: 2.0773\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 22s 204ms/step - loss: 2.0552 - val_loss: 2.0680\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 21s 196ms/step - loss: 2.0490 - val_loss: 2.0653\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 21s 202ms/step - loss: 2.0425 - val_loss: 2.0597\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 21s 201ms/step - loss: 2.0381 - val_loss: 2.0562\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 20s 192ms/step - loss: 2.0325 - val_loss: 2.0521\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 22s 208ms/step - loss: 2.0263 - val_loss: 2.0479\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 24s 228ms/step - loss: 2.0226 - val_loss: 2.0449\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 24s 229ms/step - loss: 2.0184 - val_loss: 2.0472\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 23s 218ms/step - loss: 2.0135 - val_loss: 2.0396\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 20s 193ms/step - loss: 2.0095 - val_loss: 2.0350\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 20s 194ms/step - loss: 2.0057 - val_loss: 2.0330\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 23s 218ms/step - loss: 2.0015 - val_loss: 2.0350\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 1.9982 - val_loss: 2.0322\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 23s 216ms/step - loss: 1.9935 - val_loss: 2.0278\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 23s 217ms/step - loss: 1.9908 - val_loss: 2.0247\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 23s 214ms/step - loss: 1.9871 - val_loss: 2.0252\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 22s 207ms/step - loss: 1.9847 - val_loss: 2.0204\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 21s 198ms/step - loss: 1.9820 - val_loss: 2.0201\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 19s 178ms/step - loss: 1.9785 - val_loss: 2.0179\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 19s 179ms/step - loss: 1.9755 - val_loss: 2.0150\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 18s 176ms/step - loss: 1.9718 - val_loss: 2.0177\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 19s 179ms/step - loss: 1.9713 - val_loss: 2.0181\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 1.9678 - val_loss: 2.0122\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 19s 176ms/step - loss: 1.9648 - val_loss: 2.0123\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 19s 183ms/step - loss: 1.9628 - val_loss: 2.0107\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 18s 167ms/step - loss: 1.9605 - val_loss: 2.0064\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 17s 164ms/step - loss: 1.9587 - val_loss: 2.0070\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 18s 174ms/step - loss: 1.9557 - val_loss: 2.0087\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 19s 185ms/step - loss: 1.9540 - val_loss: 2.0081\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 23s 213ms/step - loss: 1.9518 - val_loss: 2.0029\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 23s 217ms/step - loss: 1.9503 - val_loss: 2.0028\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 19s 179ms/step - loss: 1.9475 - val_loss: 2.0006\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 18s 173ms/step - loss: 1.9462 - val_loss: 2.0013\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 18s 171ms/step - loss: 1.9448 - val_loss: 2.0019\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 18s 170ms/step - loss: 1.9416 - val_loss: 2.0009\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 17s 160ms/step - loss: 1.9403 - val_loss: 1.9979\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 17s 158ms/step - loss: 1.9389 - val_loss: 1.9990\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 17s 159ms/step - loss: 1.9372 - val_loss: 1.9976\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 17s 163ms/step - loss: 1.9355 - val_loss: 1.9963\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 18s 167ms/step - loss: 1.9344 - val_loss: 1.9987\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 18s 168ms/step - loss: 1.9323 - val_loss: 1.9978\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 18s 172ms/step - loss: 1.9306 - val_loss: 1.9976\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 18s 169ms/step - loss: 1.9282 - val_loss: 1.9951\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 18s 174ms/step - loss: 1.9276 - val_loss: 1.9951\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 18s 172ms/step - loss: 1.9266 - val_loss: 1.9936\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 19s 177ms/step - loss: 1.9249 - val_loss: 1.9943\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 2.0096\n",
      "eval loss: 2.009570360183716\n",
      "perplexity 7.460111497754845\n",
      "\n",
      "по слова с париж,\n",
      "и вот такого столица.\n",
      "На коробки\n",
      "и старейский под столетный под красной\n",
      "по страна с последний старой стальным —\n",
      "и сердце на домов\n",
      "словом разве —\n",
      "стоят на двери —\n",
      "и только домов.\n",
      "\n",
      "</s>\n",
      "\n",
      "Ленин —\n",
      "не выставь\n",
      "на работая красивой.\n",
      "\n",
      "</s>\n",
      "\n",
      "Мы знаю,\n",
      "на страна —\n",
      "не собор и станет\n",
      "и столетия —\n",
      "не старья под старого стран.\n",
      "По стройки с простор,\n",
      "на столько старом не под картонки.\n",
      "\n",
      "</s>\n",
      "\n",
      "В получить на нас\n",
      "собственный страна.\n",
      "В окон за это —\n",
      "с потом на стран.\n",
      "Но с больше дома.\n",
      "На то держи —\n",
      "и стоит с пользу.\n",
      "За сто не старинский поставь —\n",
      "под мордом образительных машины.\n",
      "Под темноту\n",
      "на самого бы\n",
      "в строчки\n",
      "стороны с полезли облигации.\n",
      "Собственный кровать.\n",
      "\n",
      "</s>\n",
      "\n",
      "Только обратно.\n",
      "Под стран\n",
      "по столовый с мордобой —\n",
      "не старой громадин —\n",
      "стальный стран.\n",
      "Под страна\n",
      "с кончался стройков\n",
      "страна силы.\n",
      "Попробуй старого мало колоса.\n",
      "\n",
      "</s>\n",
      "\n",
      "В стороны\n",
      "на стих в стран.\n",
      "Так будто\n",
      "на страна\n",
      "с больше быть,\n",
      "чтоб просто\n",
      "не выставит бы —\n",
      "не смотрите на стенка —\n",
      "не на рабочих старом старом —\n",
      "до стройки,\n",
      "п\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.631321668624878\n"
     ]
    }
   ],
   "source": [
    "for embedding in embedding_dim:\n",
    "    model = MyModel(\n",
    "        vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "        embedding_dim=embedding,\n",
    "        rnn_units=rnn_units)\n",
    "    model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "    history = model.fit(train_dataset, validation_data = val_dataset, epochs=70, callbacks=[checkpoint_callback])\n",
    "    eval_loss = model.evaluate(test_dataset)\n",
    "    perplexity = np.exp(eval_loss)\n",
    "    print('eval loss:', eval_loss)\n",
    "    print('perplexity', np.exp(eval_loss))\n",
    "\n",
    "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "    start = time.time()\n",
    "    states = None\n",
    "    next_char = tf.constant(['\\n'])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(N):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    end = time.time()\n",
    "\n",
    "    result_text = result[0].numpy().decode('utf-8')\n",
    "    print(result_text)\n",
    "    print('_'*80)\n",
    "    Run_time = end - start\n",
    "    print('\\nRun time:', Run_time)\n",
    "\n",
    "    result_table_embedding_dim.fill_row(row='embedding_dim ' + str(int(embedding)), data=[eval_loss, perplexity, result_text, Run_time])\n",
    "    describe_poems_dict_embedding_dim.update({int(embedding): describe_poems(result_text)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "784930f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_rnn_units = Table(headlines=['eval loss', 'perplexity', 'result_text', 'Run time'], \n",
    "                     indexes=['rnn_units 10', 'rnn_units 100', 'rnn_units 300', 'rnn_units 500'], weighted=False)\n",
    "describe_poems_dict_rnn_units = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ae3a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Длина словаря символов\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# размерность Embedding'а\n",
    "embedding_dim = 256 #@param{type:\"number\"}\n",
    "\n",
    "# Параметры RNN-слоя\n",
    "rnn_units = [10, 100, 300, 500] #@param {type:\"number\"}\n",
    "dropout_p = 0.5\n",
    "\n",
    "T = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1859db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "102/102 [==============================] - 5s 32ms/step - loss: 4.3768 - val_loss: 3.8075\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 3.6164 - val_loss: 3.4918\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 4s 29ms/step - loss: 3.4330 - val_loss: 3.3889\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 3.3555 - val_loss: 3.3229\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 4s 32ms/step - loss: 3.2799 - val_loss: 3.2442\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 4s 32ms/step - loss: 3.2064 - val_loss: 3.1544\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 4s 32ms/step - loss: 3.1018 - val_loss: 3.0600\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.0231 - val_loss: 2.9916\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.9638 - val_loss: 2.9374\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 5s 38ms/step - loss: 2.9143 - val_loss: 2.8910\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.8725 - val_loss: 2.8534\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.8403 - val_loss: 2.8274\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.8171 - val_loss: 2.8068\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.7993 - val_loss: 2.7906\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 2.7851 - val_loss: 2.7780\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.7719 - val_loss: 2.7635\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 2.7571 - val_loss: 2.7498\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.7448 - val_loss: 2.7393\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.7325 - val_loss: 2.7263\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.7208 - val_loss: 2.7153\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.7096 - val_loss: 2.7061\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 4s 33ms/step - loss: 2.7005 - val_loss: 2.6984\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 2.6934 - val_loss: 2.6915\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 4s 33ms/step - loss: 2.6874 - val_loss: 2.6857\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.6818 - val_loss: 2.6804\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 2.6771 - val_loss: 2.6758\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.6719 - val_loss: 2.6709\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.6670 - val_loss: 2.6662\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.6630 - val_loss: 2.6629\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.6589 - val_loss: 2.6589\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.6556 - val_loss: 2.6559\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 2.6526 - val_loss: 2.6532\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 2.6495 - val_loss: 2.6502\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 2.6465 - val_loss: 2.6473\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6439 - val_loss: 2.6447\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 4s 33ms/step - loss: 2.6416 - val_loss: 2.6423\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.6396 - val_loss: 2.6404\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.6379 - val_loss: 2.6396\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 5s 39ms/step - loss: 2.6361 - val_loss: 2.6375\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 5s 39ms/step - loss: 2.6341 - val_loss: 2.6366\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 5s 37ms/step - loss: 2.6325 - val_loss: 2.6337\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 2.6313 - val_loss: 2.6335\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 5s 37ms/step - loss: 2.6297 - val_loss: 2.6325\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 5s 39ms/step - loss: 2.6281 - val_loss: 2.6312\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.6268 - val_loss: 2.6299\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.6249 - val_loss: 2.6281\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 5s 37ms/step - loss: 2.6240 - val_loss: 2.6270\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 5s 39ms/step - loss: 2.6221 - val_loss: 2.6256\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 4s 38ms/step - loss: 2.6209 - val_loss: 2.6242\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 2.6193 - val_loss: 2.6233\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 5s 43ms/step - loss: 2.6179 - val_loss: 2.6213\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 4s 33ms/step - loss: 2.6166 - val_loss: 2.6204\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6154 - val_loss: 2.6189\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6142 - val_loss: 2.6186\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6132 - val_loss: 2.6179\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6121 - val_loss: 2.6161\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6115 - val_loss: 2.6149\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6104 - val_loss: 2.6153\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6094 - val_loss: 2.6140\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6083 - val_loss: 2.6132\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6076 - val_loss: 2.6131\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6070 - val_loss: 2.6119\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 4s 32ms/step - loss: 2.6062 - val_loss: 2.6120\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6055 - val_loss: 2.6109\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6049 - val_loss: 2.6095\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6044 - val_loss: 2.6094\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6033 - val_loss: 2.6089\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 4s 31ms/step - loss: 2.6029 - val_loss: 2.6083\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6021 - val_loss: 2.6073\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 4s 30ms/step - loss: 2.6019 - val_loss: 2.6069\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 2.5952\n",
      "eval loss: 2.5952308177948\n",
      "perplexity 13.399679889051924\n",
      "\n",
      "что болать —\n",
      "на —\n",
      "вом светом позонова калить\n",
      "нальсвая сорет —\n",
      "в сто полося стерной в поде ста в води каста состо сто сто польять\n",
      "поролико года подель сте прода сто сто сто в сталь\n",
      "в сорить\n",
      "просто в калить в да нальки\n",
      "сто вы тольять\n",
      "в в ста в воранить в вореть посте сторить на на сворь с орода в семо слаз морани\n",
      "орот с дольить\n",
      "стот калиние посто стом расто\n",
      "на и нарит годет\n",
      "нане в белить\n",
      "кад\n",
      "в налом в посто полет сольить в сто в сельь в нани.\n",
      "Нать в подим в выде в продет\n",
      "нок на в снерем белеть в поросто в и не и кора подо пабот на на в кавет серит стрет в на пото в водалить ста подет сто кара разод сесто подето стенить в пода нат —\n",
      "в в поти сивот калько сто подить —\n",
      "тот в слельет\n",
      "серать вын сто в сти орок в ниль\n",
      "у пода от в пожьет стане в в сто сте заросто сте в кале сто сто ворот в подерит\n",
      "под\n",
      "попосто воде подет вылить.\n",
      "В волови —\n",
      "в в и колель весто с поле карко не дорот\n",
      "на сте слекой госто серань седа подет на —\n",
      "проба в вырой столить сто не сторь в делет,\n",
      "и побот сто в ной в сте стосто\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.0754296779632568\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 6s 45ms/step - loss: 3.4519 - val_loss: 2.9050\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 5s 43ms/step - loss: 2.7643 - val_loss: 2.6624\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.6317 - val_loss: 2.5809\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 5s 43ms/step - loss: 2.5742 - val_loss: 2.5340\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.5351 - val_loss: 2.5009\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 5s 43ms/step - loss: 2.5051 - val_loss: 2.4720\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 5s 41ms/step - loss: 2.4806 - val_loss: 2.4498\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.4601 - val_loss: 2.4304\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 5s 41ms/step - loss: 2.4429 - val_loss: 2.4160\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.4279 - val_loss: 2.4003\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.4143 - val_loss: 2.3861\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.4028 - val_loss: 2.3727\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3923 - val_loss: 2.3661\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3825 - val_loss: 2.3541\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3737 - val_loss: 2.3466\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3653 - val_loss: 2.3367\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3587 - val_loss: 2.3310\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3517 - val_loss: 2.3250\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.3453 - val_loss: 2.3182\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.3398 - val_loss: 2.3110\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3342 - val_loss: 2.3051\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.3294 - val_loss: 2.2990\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3246 - val_loss: 2.2955\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 2.3196 - val_loss: 2.2905\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 5s 41ms/step - loss: 2.3155 - val_loss: 2.2857\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 5s 43ms/step - loss: 2.3113 - val_loss: 2.2814\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 5s 42ms/step - loss: 2.3076 - val_loss: 2.2781\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 5s 42ms/step - loss: 2.3036 - val_loss: 2.2722\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 5s 42ms/step - loss: 2.3006 - val_loss: 2.2703\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 6s 58ms/step - loss: 2.2976 - val_loss: 2.2674\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 7s 56ms/step - loss: 2.2938 - val_loss: 2.2638\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2906 - val_loss: 2.2608\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 5s 48ms/step - loss: 2.2879 - val_loss: 2.2572\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2844 - val_loss: 2.2537\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2813 - val_loss: 2.2527\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 6s 51ms/step - loss: 2.2791 - val_loss: 2.2477\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 6s 50ms/step - loss: 2.2762 - val_loss: 2.2473\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 7s 59ms/step - loss: 2.2738 - val_loss: 2.2432\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 6s 51ms/step - loss: 2.2719 - val_loss: 2.2413\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.2685 - val_loss: 2.2403\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2667 - val_loss: 2.2367\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 2.2651 - val_loss: 2.2365\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 6s 49ms/step - loss: 2.2625 - val_loss: 2.2337\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 2.2600 - val_loss: 2.2337\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2587 - val_loss: 2.2303\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 5s 48ms/step - loss: 2.2560 - val_loss: 2.2270\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 6s 51ms/step - loss: 2.2553 - val_loss: 2.2264\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 6s 50ms/step - loss: 2.2534 - val_loss: 2.2239\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 6s 56ms/step - loss: 2.2514 - val_loss: 2.2225\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 2.2495 - val_loss: 2.2220\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.2481 - val_loss: 2.2192\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 6s 51ms/step - loss: 2.2465 - val_loss: 2.2195\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 5s 42ms/step - loss: 2.2452 - val_loss: 2.2176\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2433 - val_loss: 2.2133\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 2.2414 - val_loss: 2.2140\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 5s 41ms/step - loss: 2.2406 - val_loss: 2.2117\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 2.2391 - val_loss: 2.2123\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.2377 - val_loss: 2.2103\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 2.2362 - val_loss: 2.2099\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 5s 41ms/step - loss: 2.2347 - val_loss: 2.2080\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.2334 - val_loss: 2.2061\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 4s 40ms/step - loss: 2.2320 - val_loss: 2.2066\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 5s 40ms/step - loss: 2.2315 - val_loss: 2.2051\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 5s 43ms/step - loss: 2.2306 - val_loss: 2.2037\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.2295 - val_loss: 2.2040\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 6s 50ms/step - loss: 2.2280 - val_loss: 2.2003\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 5s 41ms/step - loss: 2.2262 - val_loss: 2.2000\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.2260 - val_loss: 2.1998\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 2.2247 - val_loss: 2.2006\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 2.2234 - val_loss: 2.1995\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 2.2050\n",
      "eval loss: 2.204955816268921\n",
      "perplexity 9.069850819153668\n",
      "\n",
      "на порасты и ворость,\n",
      "на нам коротом тебя громовали в под не в положенов\n",
      "на советский сердце на странах\n",
      "городится\n",
      "по красный матрительный и на вас\n",
      "не собел\n",
      "собел только странных красное морденный свете в под выстрой странный лица —\n",
      "в кровь в положеном на лениния в кировой в под летительной просторовый строком на передарить на половой в просто\n",
      "в это слова —\n",
      "на красный под простор вы стальный под менять в собором\n",
      "в кониля —\n",
      "не на станками\n",
      "под своем —\n",
      "не под остание в компарительно —\n",
      "не дома\n",
      "на советской смончины\n",
      "от возьмет на милица —\n",
      "не сторов.\n",
      "На станов,\n",
      "и молода\n",
      "в по строка облазать в под под масса.\n",
      "На машиности не выресть —\n",
      "под просто в под малок\n",
      "на мертеки и под под душали так —\n",
      "в по старию лет\n",
      "в строком —\n",
      "стальный водо не сторовой под мордет были на ветровать под под под сердце стана —\n",
      "под под провольный положенный всем прочесть по не волос\n",
      "коровает в все строго без под молодом\n",
      "в только и в тебе\n",
      "советский в под том\n",
      "в красный дело —\n",
      "под волосать в вырем —\n",
      "сторонный дома на ответ на \n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.7314255237579346\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 16s 142ms/step - loss: 3.2083 - val_loss: 2.7397\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 14s 133ms/step - loss: 2.6527 - val_loss: 2.5719\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 15s 142ms/step - loss: 2.5547 - val_loss: 2.4995\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 15s 138ms/step - loss: 2.4940 - val_loss: 2.4473\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 14s 128ms/step - loss: 2.4510 - val_loss: 2.4078\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 14s 132ms/step - loss: 2.4149 - val_loss: 2.3752\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 14s 132ms/step - loss: 2.3842 - val_loss: 2.3478\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 13s 126ms/step - loss: 2.3554 - val_loss: 2.3203\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 14s 131ms/step - loss: 2.3298 - val_loss: 2.2968\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 14s 129ms/step - loss: 2.3059 - val_loss: 2.2723\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 14s 129ms/step - loss: 2.2853 - val_loss: 2.2535\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 13s 127ms/step - loss: 2.2658 - val_loss: 2.2375\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 13s 123ms/step - loss: 2.2488 - val_loss: 2.2168\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.2320 - val_loss: 2.2037\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 13s 126ms/step - loss: 2.2180 - val_loss: 2.1916\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 14s 128ms/step - loss: 2.2046 - val_loss: 2.1802\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 2.1923 - val_loss: 2.1676\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.1803 - val_loss: 2.1572\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 2.1705 - val_loss: 2.1483\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 13s 120ms/step - loss: 2.1596 - val_loss: 2.1401\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 14s 128ms/step - loss: 2.1519 - val_loss: 2.1367\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 2.1434 - val_loss: 2.1284\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.1347 - val_loss: 2.1212\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.1264 - val_loss: 2.1138\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.1200 - val_loss: 2.1079\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 2.1132 - val_loss: 2.1019\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.1064 - val_loss: 2.0975\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.1008 - val_loss: 2.0929\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0952 - val_loss: 2.0907\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0902 - val_loss: 2.0827\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0847 - val_loss: 2.0816\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 13s 118ms/step - loss: 2.0788 - val_loss: 2.0743\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0751 - val_loss: 2.0706\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0702 - val_loss: 2.0702\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0665 - val_loss: 2.0662\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 2.0625 - val_loss: 2.0611\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0572 - val_loss: 2.0581\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0529 - val_loss: 2.0584\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0510 - val_loss: 2.0506\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0462 - val_loss: 2.0517\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0437 - val_loss: 2.0508\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 2.0393 - val_loss: 2.0446\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0366 - val_loss: 2.0454\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0340 - val_loss: 2.0459\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0307 - val_loss: 2.0390\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 2.0281 - val_loss: 2.0409\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 12s 118ms/step - loss: 2.0246 - val_loss: 2.0360\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0224 - val_loss: 2.0334\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0194 - val_loss: 2.0338\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0168 - val_loss: 2.0326\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0141 - val_loss: 2.0261\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0121 - val_loss: 2.0281\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0092 - val_loss: 2.0243\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0076 - val_loss: 2.0222\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 2.0047 - val_loss: 2.0213\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0023 - val_loss: 2.0215\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 2.0014 - val_loss: 2.0212\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 12s 113ms/step - loss: 1.9984 - val_loss: 2.0187\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 1.9967 - val_loss: 2.0164\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9957 - val_loss: 2.0208\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9921 - val_loss: 2.0138\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9914 - val_loss: 2.0120\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9885 - val_loss: 2.0157\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 12s 115ms/step - loss: 1.9878 - val_loss: 2.0135\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9860 - val_loss: 2.0117\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9844 - val_loss: 2.0122\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 12s 113ms/step - loss: 1.9831 - val_loss: 2.0128\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 12s 114ms/step - loss: 1.9824 - val_loss: 2.0141\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 13s 121ms/step - loss: 1.9799 - val_loss: 2.0067\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 12s 116ms/step - loss: 1.9783 - val_loss: 2.0066\n",
      "12/12 [==============================] - 1s 38ms/step - loss: 2.0232\n",
      "eval loss: 2.0232291221618652\n",
      "perplexity 7.562706450030004\n",
      "\n",
      "Советский под нам\n",
      "не до полез дело —\n",
      "делам\n",
      "с под воздух,\n",
      "по своей трудом\n",
      "в морда\n",
      "и в дверь\n",
      "на столовой старья.\n",
      "Под место весь —\n",
      "на старый не старом столетье.\n",
      "У нас\n",
      "на детишь —\n",
      "под немень под простой!\n",
      "\n",
      "</s>\n",
      "\n",
      "Под старой стальным дело\n",
      "не за нас\n",
      "да на сто словом.\n",
      "Собственный домов.\n",
      "\n",
      "\n",
      "В страницы под нам —\n",
      "и то старье с благороды.\n",
      "\n",
      "\n",
      "В рабочий\n",
      "в рабочих труд,\n",
      "все меня делать кончики\n",
      "в облакать\n",
      "и вот в белый руководительных рабочий миром,\n",
      "как за каждый дома\n",
      "на каждому столетие,\n",
      "по старами\n",
      "в каждый темя,\n",
      "свои\n",
      "от старое будет в облака,\n",
      "не под полесать —\n",
      "в столетной старой страницы.\n",
      "В одно на великих\n",
      "под старой под нам\n",
      "под немедленный куры труда.\n",
      "Не стоит ли бы\n",
      "на место старой.\n",
      "Но с того последний в город,\n",
      "как на под него молодежь,\n",
      "как в общество\n",
      "на дело —\n",
      "не по себе\n",
      "по стены\n",
      "с небесных под нашей уже\n",
      "на столовых\n",
      "собственной крестьянин,\n",
      "на устальным в коммунистом.\n",
      "С немедленных железного лица,\n",
      "рабочий компания.\n",
      "Смотрите, как в ответ,\n",
      "на красным в полтам,\n",
      "в миллионные глаза\n",
      "в грязник старой полевет\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.9760453701019287\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 24s 218ms/step - loss: 3.1880 - val_loss: 2.7396\n",
      "Epoch 2/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 2.6563 - val_loss: 2.5716\n",
      "Epoch 3/70\n",
      "102/102 [==============================] - 23s 221ms/step - loss: 2.5563 - val_loss: 2.5053\n",
      "Epoch 4/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 2.4958 - val_loss: 2.4474\n",
      "Epoch 5/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.4503 - val_loss: 2.4012\n",
      "Epoch 6/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.4065 - val_loss: 2.3653\n",
      "Epoch 7/70\n",
      "102/102 [==============================] - 22s 214ms/step - loss: 2.3688 - val_loss: 2.3319\n",
      "Epoch 8/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 2.3347 - val_loss: 2.2996\n",
      "Epoch 9/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.3012 - val_loss: 2.2685\n",
      "Epoch 10/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.2708 - val_loss: 2.2432\n",
      "Epoch 11/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.2433 - val_loss: 2.2120\n",
      "Epoch 12/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 2.2189 - val_loss: 2.1944\n",
      "Epoch 13/70\n",
      "102/102 [==============================] - 23s 219ms/step - loss: 2.1960 - val_loss: 2.1735\n",
      "Epoch 14/70\n",
      "102/102 [==============================] - 22s 214ms/step - loss: 2.1750 - val_loss: 2.1537\n",
      "Epoch 15/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.1555 - val_loss: 2.1338\n",
      "Epoch 16/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.1382 - val_loss: 2.1228\n",
      "Epoch 17/70\n",
      "102/102 [==============================] - 23s 217ms/step - loss: 2.1244 - val_loss: 2.1167\n",
      "Epoch 18/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.1085 - val_loss: 2.1059\n",
      "Epoch 19/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.0953 - val_loss: 2.0975\n",
      "Epoch 20/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.0837 - val_loss: 2.0860\n",
      "Epoch 21/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.0716 - val_loss: 2.0797\n",
      "Epoch 22/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.0621 - val_loss: 2.0708\n",
      "Epoch 23/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 2.0501 - val_loss: 2.0604\n",
      "Epoch 24/70\n",
      "102/102 [==============================] - 22s 210ms/step - loss: 2.0412 - val_loss: 2.0562\n",
      "Epoch 25/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 2.0324 - val_loss: 2.0532\n",
      "Epoch 26/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 2.0238 - val_loss: 2.0439\n",
      "Epoch 27/70\n",
      "102/102 [==============================] - 22s 209ms/step - loss: 2.0154 - val_loss: 2.0440\n",
      "Epoch 28/70\n",
      "102/102 [==============================] - 22s 214ms/step - loss: 2.0075 - val_loss: 2.0336\n",
      "Epoch 29/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9991 - val_loss: 2.0288\n",
      "Epoch 30/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 1.9933 - val_loss: 2.0296\n",
      "Epoch 31/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9874 - val_loss: 2.0188\n",
      "Epoch 32/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9801 - val_loss: 2.0228\n",
      "Epoch 33/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9739 - val_loss: 2.0142\n",
      "Epoch 34/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9679 - val_loss: 2.0132\n",
      "Epoch 35/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9633 - val_loss: 2.0137\n",
      "Epoch 36/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9578 - val_loss: 2.0072\n",
      "Epoch 37/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9529 - val_loss: 2.0085\n",
      "Epoch 38/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9483 - val_loss: 2.0032\n",
      "Epoch 39/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9433 - val_loss: 2.0029\n",
      "Epoch 40/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9385 - val_loss: 2.0006\n",
      "Epoch 41/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 1.9346 - val_loss: 1.9980\n",
      "Epoch 42/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9293 - val_loss: 2.0001\n",
      "Epoch 43/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.9257 - val_loss: 1.9926\n",
      "Epoch 44/70\n",
      "102/102 [==============================] - 22s 210ms/step - loss: 1.9215 - val_loss: 1.9926\n",
      "Epoch 45/70\n",
      "102/102 [==============================] - 22s 210ms/step - loss: 1.9159 - val_loss: 1.9948\n",
      "Epoch 46/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 1.9141 - val_loss: 1.9940\n",
      "Epoch 47/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9105 - val_loss: 1.9897\n",
      "Epoch 48/70\n",
      "102/102 [==============================] - 22s 210ms/step - loss: 1.9076 - val_loss: 1.9872\n",
      "Epoch 49/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9033 - val_loss: 1.9840\n",
      "Epoch 50/70\n",
      "102/102 [==============================] - 22s 210ms/step - loss: 1.8997 - val_loss: 1.9834\n",
      "Epoch 51/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8962 - val_loss: 1.9890\n",
      "Epoch 52/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.8930 - val_loss: 1.9849\n",
      "Epoch 53/70\n",
      "102/102 [==============================] - 22s 209ms/step - loss: 1.8914 - val_loss: 1.9844\n",
      "Epoch 54/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8877 - val_loss: 1.9875\n",
      "Epoch 55/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8848 - val_loss: 1.9847\n",
      "Epoch 56/70\n",
      "102/102 [==============================] - 22s 214ms/step - loss: 1.8830 - val_loss: 1.9811\n",
      "Epoch 57/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 1.8794 - val_loss: 1.9822\n",
      "Epoch 58/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8782 - val_loss: 1.9821\n",
      "Epoch 59/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8759 - val_loss: 1.9798\n",
      "Epoch 60/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.8710 - val_loss: 1.9782\n",
      "Epoch 61/70\n",
      "102/102 [==============================] - 22s 214ms/step - loss: 1.8690 - val_loss: 1.9775\n",
      "Epoch 62/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8668 - val_loss: 1.9798\n",
      "Epoch 63/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 1.8641 - val_loss: 1.9767\n",
      "Epoch 64/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 1.8632 - val_loss: 1.9830\n",
      "Epoch 65/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.8622 - val_loss: 1.9818\n",
      "Epoch 66/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.8573 - val_loss: 1.9804\n",
      "Epoch 67/70\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.8571 - val_loss: 1.9803\n",
      "Epoch 68/70\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8550 - val_loss: 1.9810\n",
      "Epoch 69/70\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 1.8509 - val_loss: 1.9780\n",
      "Epoch 70/70\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 1.8508 - val_loss: 1.9771\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 1.9863\n",
      "eval loss: 1.9862890243530273\n",
      "perplexity 7.288436304772888\n",
      "\n",
      "и столетья,\n",
      "под носильный стране.\n",
      "Слово старья —\n",
      "не сказал —\n",
      "не выставь на всех подошла —\n",
      "просто\n",
      "просто\n",
      "по стенем\n",
      "в ломанин\n",
      "словом страны советской страницы.\n",
      "Будет верный,\n",
      "не под носильной получиться.\n",
      "Много столетний полиций.\n",
      "Советской полоса.\n",
      "Стальные вода.\n",
      "\n",
      "</s>\n",
      "\n",
      "Мы самогоном корова.\n",
      "Но только скажет:\n",
      "«Не спитались в селе\n",
      "по стены\n",
      "в польши в страницы.\n",
      "Стойт положиться —\n",
      "не сказал,\n",
      "на поле весело.\n",
      "Все просто —\n",
      "это значит,\n",
      "что слово стараясь,\n",
      "скультурной рукой.\n",
      "И вот так же\n",
      "возьмем просто\n",
      "под нам\n",
      "не скрипить на стеных стране.\n",
      "Стальные страна,\n",
      "чтоб все в первый получиться,\n",
      "с под странам\n",
      "с под ногами\n",
      "стройки\n",
      "с трудом подымают мир медведь —\n",
      "на стеному\n",
      "под нами —\n",
      "пока\n",
      "построит\n",
      "От бумажный петь.\n",
      "На будто он —\n",
      "старается\n",
      "в коммунистические улицы.\n",
      "За водком\n",
      "силы\n",
      "в столе\n",
      "не должно быть\n",
      "в стеных перед нам\n",
      "не скоро\n",
      "не смотрите под нам —\n",
      "по стране\n",
      "в молодежь,\n",
      "скользки стараясь.\n",
      "Много столетий\n",
      "самогоном голову —\n",
      "и вот так же\n",
      "в семейство\n",
      "и стальные спинами\n",
      "стальные под ног.\n",
      "Как будто\n",
      "стальные спорт.\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.0219297409057617\n"
     ]
    }
   ],
   "source": [
    "for rnn_un in rnn_units:\n",
    "    model = MyModel(\n",
    "        vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_units=rnn_un)\n",
    "    model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "    history = model.fit(train_dataset, validation_data = val_dataset, epochs=70, callbacks=[checkpoint_callback])\n",
    "    eval_loss = model.evaluate(test_dataset)\n",
    "    perplexity = np.exp(eval_loss)\n",
    "    print('eval loss:', eval_loss)\n",
    "    print('perplexity', np.exp(eval_loss))\n",
    "\n",
    "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "    start = time.time()\n",
    "    states = None\n",
    "    next_char = tf.constant(['\\n'])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(N):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    end = time.time()\n",
    "\n",
    "    result_text = result[0].numpy().decode('utf-8')\n",
    "    print(result_text)\n",
    "    print('_'*80)\n",
    "    Run_time = end - start\n",
    "    print('\\nRun time:', Run_time)\n",
    "\n",
    "    result_table_rnn_units.fill_row(row='rnn_units ' + str(int(rnn_un)), data=[eval_loss, perplexity, result_text, Run_time])\n",
    "    describe_poems_dict_rnn_units.update({int(rnn_un): describe_poems(result_text)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d7453",
   "metadata": {},
   "source": [
    "# Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185b725",
   "metadata": {},
   "source": [
    "### Таблица по сравнению эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03442b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>result_text</th>\n",
       "      <th>Run time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EPOCHS 5</th>\n",
       "      <td>2.405268</td>\n",
       "      <td>11.081404</td>\n",
       "      <td>\\nВсем бростая.\\nВ странной в долонки.\\n\\n&lt;/s&gt;...</td>\n",
       "      <td>1.040562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPOCHS 15</th>\n",
       "      <td>2.144165</td>\n",
       "      <td>8.534912</td>\n",
       "      <td>\\nне света\\nна положенный стой!\\nХоть на нет н...</td>\n",
       "      <td>0.905763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPOCHS 30</th>\n",
       "      <td>2.041177</td>\n",
       "      <td>7.699665</td>\n",
       "      <td>\\nодно сказать в гром деньгу дома\\nне выставил...</td>\n",
       "      <td>1.061969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPOCHS 50</th>\n",
       "      <td>2.003134</td>\n",
       "      <td>7.412246</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nВот —\\nи простоло\\nи в разумее...</td>\n",
       "      <td>1.284416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPOCHS 70</th>\n",
       "      <td>1.997716</td>\n",
       "      <td>7.372198</td>\n",
       "      <td>\\nи в старом востока\\nи в стране\\nсветите\\nне ...</td>\n",
       "      <td>1.361585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          eval loss perplexity  \\\n",
       "EPOCHS 5   2.405268  11.081404   \n",
       "EPOCHS 15  2.144165   8.534912   \n",
       "EPOCHS 30  2.041177   7.699665   \n",
       "EPOCHS 50  2.003134   7.412246   \n",
       "EPOCHS 70  1.997716   7.372198   \n",
       "\n",
       "                                                 result_text  Run time  \n",
       "EPOCHS 5   \\nВсем бростая.\\nВ странной в долонки.\\n\\n</s>...  1.040562  \n",
       "EPOCHS 15  \\nне света\\nна положенный стой!\\nХоть на нет н...  0.905763  \n",
       "EPOCHS 30  \\nодно сказать в гром деньгу дома\\nне выставил...  1.061969  \n",
       "EPOCHS 50  \\n\\n\\n\\n\\n\\n\\n\\nВот —\\nи простоло\\nи в разумее...  1.284416  \n",
       "EPOCHS 70  \\nи в старом востока\\nи в стране\\nсветите\\nне ...  1.361585  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_EPOCHS.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df1f20d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5:               len      lines  mean_line_len\n",
       " count    2.000000   2.000000       2.000000\n",
       " mean   496.000000  28.000000      16.553571\n",
       " std    651.952452  38.183766       0.631345\n",
       " min     35.000000   1.000000      16.107143\n",
       " 25%    265.500000  14.500000      16.330357\n",
       " 50%    496.000000  28.000000      16.553571\n",
       " 75%    726.500000  41.500000      16.776786\n",
       " max    957.000000  55.000000      17.000000,\n",
       " 15:           len  lines  mean_line_len\n",
       " count     1.0    1.0           1.00\n",
       " mean   1000.0   53.0          18.94\n",
       " std       NaN    NaN            NaN\n",
       " min    1000.0   53.0          18.94\n",
       " 25%    1000.0   53.0          18.94\n",
       " 50%    1000.0   53.0          18.94\n",
       " 75%    1000.0   53.0          18.94\n",
       " max    1000.0   53.0          18.94,\n",
       " 30:               len      lines  mean_line_len\n",
       " count    3.000000   3.000000       3.000000\n",
       " mean   328.000000  14.333333      19.807217\n",
       " std    233.231645  10.692677       3.697668\n",
       " min    101.000000   5.000000      16.000000\n",
       " 25%    208.500000   8.500000      18.018519\n",
       " 50%    316.000000  12.000000      20.037037\n",
       " 75%    441.500000  19.000000      21.710826\n",
       " max    567.000000  26.000000      23.384615,\n",
       " 50:          len  lines  mean_line_len\n",
       " count    1.0    1.0       1.000000\n",
       " mean   993.0   61.0      15.032258\n",
       " std      NaN    NaN            NaN\n",
       " min    993.0   61.0      15.032258\n",
       " 25%    993.0   61.0      15.032258\n",
       " 50%    993.0   61.0      15.032258\n",
       " 75%    993.0   61.0      15.032258\n",
       " max    993.0   61.0      15.032258,\n",
       " 70:               len      lines  mean_line_len\n",
       " count    3.000000   3.000000       3.000000\n",
       " mean   328.000000  16.000000      18.432900\n",
       " std    304.471674  15.620499       4.459309\n",
       " min    101.000000   6.000000      13.571429\n",
       " 25%    155.000000   7.000000      16.482684\n",
       " 50%    209.000000   8.000000      19.393939\n",
       " 75%    441.500000  21.000000      20.863636\n",
       " max    674.000000  34.000000      22.333333}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_poems_dict_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c7c78",
   "metadata": {},
   "source": [
    "### Таблица по сравнению embedding_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed5c8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>result_text</th>\n",
       "      <th>Run time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>embedding_dim 35</th>\n",
       "      <td>2.155928</td>\n",
       "      <td>8.635904</td>\n",
       "      <td>\\nс дело\\nпо стальный под под страна.\\nПостопл...</td>\n",
       "      <td>1.147767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding_dim 70</th>\n",
       "      <td>2.087182</td>\n",
       "      <td>8.062162</td>\n",
       "      <td>\\nне все в забетать в страно —\\nи столет и в к...</td>\n",
       "      <td>1.668085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding_dim 140</th>\n",
       "      <td>2.050038</td>\n",
       "      <td>7.768195</td>\n",
       "      <td>\\nв страна\\nс порох,\\nпока\\nне под ногам\\nв но...</td>\n",
       "      <td>1.899963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding_dim 280</th>\n",
       "      <td>2.024001</td>\n",
       "      <td>7.568549</td>\n",
       "      <td>\\nпоставить на всегда —\\nв под вас\\nне в стран...</td>\n",
       "      <td>1.544119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding_dim 560</th>\n",
       "      <td>2.00957</td>\n",
       "      <td>7.460111</td>\n",
       "      <td>\\nпо слова с париж,\\nи вот такого столица.\\nНа...</td>\n",
       "      <td>1.631322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  eval loss perplexity  \\\n",
       "embedding_dim 35   2.155928   8.635904   \n",
       "embedding_dim 70   2.087182   8.062162   \n",
       "embedding_dim 140  2.050038   7.768195   \n",
       "embedding_dim 280  2.024001   7.568549   \n",
       "embedding_dim 560   2.00957   7.460111   \n",
       "\n",
       "                                                         result_text  Run time  \n",
       "embedding_dim 35   \\nс дело\\nпо стальный под под страна.\\nПостопл...  1.147767  \n",
       "embedding_dim 70   \\nне все в забетать в страно —\\nи столет и в к...  1.668085  \n",
       "embedding_dim 140  \\nв страна\\nс порох,\\nпока\\nне под ногам\\nв но...  1.899963  \n",
       "embedding_dim 280  \\nпоставить на всегда —\\nв под вас\\nне в стран...  1.544119  \n",
       "embedding_dim 560  \\nпо слова с париж,\\nи вот такого столица.\\nНа...  1.631322  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_embedding_dim.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3651ac1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35:               len      lines  mean_line_len\n",
       " count    5.000000   5.000000       5.000000\n",
       " mean   193.600000   8.600000      18.291823\n",
       " std    162.653927   7.503333       1.849337\n",
       " min     33.000000   1.000000      16.000000\n",
       " 25%     77.000000   3.000000      16.857143\n",
       " 50%    124.000000   6.000000      18.500000\n",
       " 75%    340.000000  15.000000      19.789474\n",
       " max    394.000000  18.000000      20.312500,\n",
       " 70:               len      lines  mean_line_len\n",
       " count    2.000000   2.000000       2.000000\n",
       " mean   496.000000  24.000000      22.098485\n",
       " std    470.933116  26.870058       5.988980\n",
       " min    163.000000   5.000000      17.863636\n",
       " 25%    329.500000  14.500000      19.981061\n",
       " 50%    496.000000  24.000000      22.098485\n",
       " 75%    662.500000  33.500000      24.215909\n",
       " max    829.000000  43.000000      26.333333,\n",
       " 140:               len      lines  mean_line_len\n",
       " count    3.000000   3.000000       3.000000\n",
       " mean   328.000000  17.666667      16.975678\n",
       " std    230.922065  13.650397       0.928848\n",
       " min     73.000000   3.000000      15.903226\n",
       " 25%    230.500000  11.500000      16.701613\n",
       " 50%    388.000000  20.000000      17.500000\n",
       " 75%    455.500000  25.000000      17.511905\n",
       " max    523.000000  30.000000      17.523810,\n",
       " 280:               len     lines  mean_line_len\n",
       " count    2.000000   2.00000       2.000000\n",
       " mean   496.000000  22.00000      19.747748\n",
       " std    456.790981  19.79899       2.000284\n",
       " min    173.000000   8.00000      18.333333\n",
       " 25%    334.500000  15.00000      19.040541\n",
       " 50%    496.000000  22.00000      19.747748\n",
       " 75%    657.500000  29.00000      20.454955\n",
       " max    819.000000  36.00000      21.162162,\n",
       " 560:               len      lines  mean_line_len\n",
       " count    6.000000   6.000000       6.000000\n",
       " mean   160.000000   7.833333      16.772848\n",
       " std     75.498344   3.710346       3.258547\n",
       " min     39.000000   2.000000      12.333333\n",
       " 25%    142.250000   6.500000      14.340909\n",
       " 50%    159.500000   8.000000      17.678571\n",
       " 75%    186.500000   9.500000      18.625000\n",
       " max    270.000000  13.000000      20.777778}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_poems_dict_embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcd887",
   "metadata": {},
   "source": [
    "### Таблица по сравнению rnn_units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a8bc596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>result_text</th>\n",
       "      <th>Run time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rnn_units 10</th>\n",
       "      <td>2.595231</td>\n",
       "      <td>13.39968</td>\n",
       "      <td>\\nчто болать —\\nна —\\nвом светом позонова кали...</td>\n",
       "      <td>1.07543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_units 100</th>\n",
       "      <td>2.204956</td>\n",
       "      <td>9.069851</td>\n",
       "      <td>\\nна порасты и ворость,\\nна нам коротом тебя г...</td>\n",
       "      <td>1.731426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_units 300</th>\n",
       "      <td>2.023229</td>\n",
       "      <td>7.562706</td>\n",
       "      <td>\\nСоветский под нам\\nне до полез дело —\\nделам...</td>\n",
       "      <td>0.976045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_units 500</th>\n",
       "      <td>1.986289</td>\n",
       "      <td>7.288436</td>\n",
       "      <td>\\nи столетья,\\nпод носильный стране.\\nСлово ст...</td>\n",
       "      <td>1.02193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              eval loss perplexity  \\\n",
       "rnn_units 10   2.595231   13.39968   \n",
       "rnn_units 100  2.204956   9.069851   \n",
       "rnn_units 300  2.023229   7.562706   \n",
       "rnn_units 500  1.986289   7.288436   \n",
       "\n",
       "                                                     result_text  Run time  \n",
       "rnn_units 10   \\nчто болать —\\nна —\\nвом светом позонова кали...   1.07543  \n",
       "rnn_units 100  \\nна порасты и ворость,\\nна нам коротом тебя г...  1.731426  \n",
       "rnn_units 300  \\nСоветский под нам\\nне до полез дело —\\nделам...  0.976045  \n",
       "rnn_units 500  \\nи столетья,\\nпод носильный стране.\\nСлово ст...   1.02193  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_rnn_units.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17f61510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10:           len  lines  mean_line_len\n",
       " count     1.0    1.0       1.000000\n",
       " mean   1000.0   28.0      33.517241\n",
       " std       NaN    NaN            NaN\n",
       " min    1000.0   28.0      33.517241\n",
       " 25%    1000.0   28.0      33.517241\n",
       " 50%    1000.0   28.0      33.517241\n",
       " 75%    1000.0   28.0      33.517241\n",
       " max    1000.0   28.0      33.517241,\n",
       " 100:          len  lines  mean_line_len\n",
       " count    1.0    1.0       1.000000\n",
       " mean   999.0   33.0      28.411765\n",
       " std      NaN    NaN            NaN\n",
       " min    999.0   33.0      28.411765\n",
       " 25%    999.0   33.0      28.411765\n",
       " 50%    999.0   33.0      28.411765\n",
       " 75%    999.0   33.0      28.411765\n",
       " max    999.0   33.0      28.411765,\n",
       " 300:               len     lines  mean_line_len\n",
       " count    2.000000   2.00000       2.000000\n",
       " mean   496.000000  27.50000      16.567308\n",
       " std    420.021428  21.92031       3.086793\n",
       " min    199.000000  12.00000      14.384615\n",
       " 25%    347.500000  19.75000      15.475962\n",
       " 50%    496.000000  27.50000      16.567308\n",
       " 75%    644.500000  35.25000      17.658654\n",
       " max    793.000000  43.00000      18.750000,\n",
       " 500:               len      lines  mean_line_len\n",
       " count    2.000000   2.000000       2.000000\n",
       " mean   496.000000  30.500000      15.289583\n",
       " std    336.582828  23.334524       1.381805\n",
       " min    258.000000  14.000000      14.312500\n",
       " 25%    377.000000  22.250000      14.801042\n",
       " 50%    496.000000  30.500000      15.289583\n",
       " 75%    615.000000  38.750000      15.778125\n",
       " max    734.000000  47.000000      16.266667}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_poems_dict_rnn_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce23dc",
   "metadata": {},
   "source": [
    "# Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cd2eb",
   "metadata": {},
   "source": [
    "`Пункт 6. Изменение температуры`\n",
    "Визуально сложно определить, какое значение температуры лучше, в дальнейшем использовалось значение T=0.3\n",
    "\n",
    "`Пункт 7. Изменение эмбеддинга`\n",
    "*Эмбеддинг (embedding) — это процесс представления слов или фраз в виде векторов фиксированной длины.* При увеличении размерности вектора уменьшалась перплексия и улучшалось качество генерации текста, время на обучение увеличивалось.\n",
    "\n",
    "\n",
    "`Пункт 7. Изменение параметра units в tf.keras.layers.SimpleRNN`\n",
    "*Параметр units в слое tf.keras.layers.SimpleRNN определяет количество нейронов (или размерность скрытого состояния) в рекуррентном слое. Каждый нейрон в слое обрабатывает входные данные и генерирует скрытое состояние, которое передается на следующий шаг обработки.* При увеличении числа нейронов уменьшалась перплексия и улучшалось качество генерации текста, увеличивалось время обучения. Однако стоит понимать, что бесконечное увелечение нейронов может привести к переобучению.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Перплексия (perplexity) — это метрика, используемая для оценки качества языковых моделей. Она показывает, насколько хорошо модель может предсказать следующее слово в тексте, и вычисляется как экспонента энтропии на текстовой корпус.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c30690ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_table_EPOCHS.table()\n",
    "df.to_excel(\"result_table_EPOCHS.xlsx\")\n",
    "df = result_table_embedding_dim.table()\n",
    "df.to_excel(\"result_table_embedding_dim.xlsx\")\n",
    "df = result_table_rnn_units.table()\n",
    "df.to_excel(\"result_table_rnn_units.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4707bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # открытие файла для записи\n",
    "# file = open('my_dict.json', 'r')\n",
    "# # закрытие файла\n",
    "# file.close()\n",
    "\n",
    "\n",
    "# import json\n",
    "\n",
    "# # создание словаря\n",
    "# my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
    "\n",
    "# # сохранение словаря в файл\n",
    "# with open('my_dict.json', 'w') as file:\n",
    "#     json.dump(my_dict, file)\n",
    "\n",
    "    \n",
    "# import json\n",
    "\n",
    "# # чтение словаря из файла\n",
    "# with open('my_dict.json', 'r') as file:\n",
    "#     my_dict = json.load(file)\n",
    "\n",
    "\n",
    "# # вывод словаря\n",
    "# print(my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58dcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
