{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c07acd2",
   "metadata": {},
   "source": [
    "# Лаба №2 Козлов, Ярикова. А-03 moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3dc06",
   "metadata": {},
   "source": [
    "| Вариант | Класс     |\n",
    "|:---------:|:-----------:|\n",
    "| 9       | 6, 17, 19 |\n",
    "\n",
    "| № класса | Название класса          |\n",
    "|:----------:|:--------------------------:|\n",
    "| 6        | 'comp.windows.x'  |\n",
    "| 17       | 'talk.politics.guns' |\n",
    "| 19       | 'talk.politics.misc' |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4385e",
   "metadata": {},
   "source": [
    "# П2. Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4e9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908bef6",
   "metadata": {},
   "source": [
    "# П3. Загрузка выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c94ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.windows.x', 'talk.politics.guns', 'talk.politics.misc'] \n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories, remove=remove)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d8897",
   "metadata": {},
   "source": [
    "# П4. Вывести на экран по одному-два документа каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74bc64c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "[1 2 0 ... 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.keys())\n",
    "print(twenty_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98d6ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.windows.x\n",
      "Hi, looking for any advice or suggestions about a problem I'm\n",
      "having with MIT X11R5's editres, in particular under twm variants.\n",
      "\n",
      "For a start, 9 times out of 10 (but NOT always) editres won't\n",
      "grab a widget tree when running on our NCD (Decwindows) Xterms,\n",
      "which I'm told will be fixed when the R5 (not R4) XDm is installed.\n",
      "OK, so I tried running it on a Sun, running real R5, on the same\n",
      "network - I get a widget tree, but it's ALWAYS for 'TWM Icon Manager'\n",
      "\n",
      "Anybody know of any patches for (a) twm or (b) editres that I should\n",
      "look at?\n",
      "----------------------------------\n",
      "\n",
      "talk.politics.guns\n",
      "\n",
      "That's open for debate.  Certainly, an excessive number of people are\n",
      "murdered every year but people also do save innocent lives with firearms.\n",
      "The media just don't tell us when it happens...\n",
      "\n",
      "\n",
      "I think there are more of us than there are federal marshalls...\n",
      "\n",
      "\n",
      "Crap.  It's simplistic thinking on the part of feather-headed dolts.\n",
      "\n",
      "\n",
      "Nuts.\n",
      "----------------------------------\n",
      "\n",
      "talk.politics.misc\n",
      "\n",
      "Once again, it appears that the one-eyed man has appeared in the land of the sighted\n",
      "and for some strange resaon has appointed himself the ruler and supreme power.\n",
      "----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train['target_names'][0], twenty_train['data'][list(twenty_train['target']).index(0)], sep='\\n',\n",
    "      end=\"\\n----------------------------------\\n\\n\")\n",
    "print(twenty_train['target_names'][1], twenty_train['data'][list(twenty_train['target']).index(1)], sep='\\n',\n",
    "      end=\"\\n----------------------------------\\n\\n\")\n",
    "print(twenty_train['target_names'][2], twenty_train['data'][list(twenty_train['target']).index(2)], sep='\\n',\n",
    "      end=\"\\n----------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87030ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\u522-16\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38513157",
   "metadata": {},
   "source": [
    "# П5. Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db44341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "from nltk import word_tokenize\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stem_train = []; stem_test = []\n",
    "for text in twenty_train.data:\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    line = ''\n",
    "    for word in nltk_tokens:\n",
    "        line += ' ' + porter_stemmer.stem(word)\n",
    "    stem_train.append(line)\n",
    "\n",
    "for text in twenty_test.data:\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    line = ''\n",
    "    for word in nltk_tokens:\n",
    "        line += ' ' + porter_stemmer.stem(word)\n",
    "    stem_test.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc9e0c",
   "metadata": {},
   "source": [
    "# П6. Векторизация\n",
    "**Описание *CountVectorizer*:**\n",
    "\n",
    "__*CountVectorizer*__ means breaking down a sentence or any text into words by performing preprocessing tasks like converting all words to lowercase, thus removing special characters. In NLP models can't understand textual data they only accept numbers, so this textual data needs to be vectorized.\n",
    "\n",
    "__*CountVectorizer*__ означает разбиение предложения или любого текста на слова путем выполнения задач предварительной обработки, таких как преобразование всех слов в нижний регистр и удаление специальных символов. В моделях NLP не могут понимать текстовые данные, они принимают только числа, поэтому эти текстовые данные необходимо векторизовать.\n",
    "\n",
    "Предобработка текста, токенизация и отбрасывание стоп-слов включены в состав модуля __*CountVectorizer*__, который позволяет создать словарь характерных признаков и перевести документы в векторы признаков.\n",
    "\n",
    "## без отсечения стоп-слов и без стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f053a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features = 10000, stop_words = None)\n",
    "\n",
    "vect.fit(twenty_train.data)  # Создание векторного словаря признаков\n",
    "\n",
    "train_data = vect.transform(twenty_train.data)  # Перевод текста в векторный вид\n",
    "test_data = vect.transform(twenty_test.data)  # Перевод текста в векторный вид"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9876b",
   "metadata": {},
   "source": [
    "### Вывод первых 20-ти слов по частоте встречаемости\n",
    "### Коэффициент Жаккара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a956d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а — количество видов на первой пробной площадке, b — количество видов на второй пробной площадке, с — количество видов, общих для 1-й и 2-й площадок.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "image/png": {
       "height": 240,
       "width": 320
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "print('а — количество видов на первой пробной площадке, b — количество видов на второй пробной площадке, с — количество видов, общих для 1-й и 2-й площадок.')\n",
    "Image(\"image/Jaccard.png\", width=320, height=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9e2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frequency_words(train, test, class_num=None, stop_words=None):\n",
    "\n",
    "    if class_num is not None:\n",
    "        train = [twenty_train.data[i] for i in np.where(twenty_train.target == class_num)[0]]\n",
    "\n",
    "    vect = CountVectorizer(max_features = 10000, stop_words = stop_words)\n",
    "    train_data = vect.fit_transform(train)\n",
    "\n",
    "    x = list(zip(vect.get_feature_names_out(), np.ravel(train_data.sum(axis=0))))\n",
    "    x.sort(key=lambda row: row[1], reverse=True) \n",
    "    if class_num is not None:\n",
    "        print(f'Первые 20 наиболее частотных слов {class_num} класса:')\n",
    "    else:\n",
    "        print(f'Первые 20 наиболее частотных слов всей выборки:')\n",
    "#     [print(x[i]) for i in range(20)]\n",
    "    print(x[:20])\n",
    "    \n",
    "def calc_jaccard_coef(first_list, second_list):\n",
    "    '''Рассчет коэффициента Жаккара'''\n",
    "    first_set = set(first_list)\n",
    "    second_set = set(second_list)\n",
    "    total = len(first_set.intersection(second_set))\n",
    "    coef = total / (len(first_set) + len(second_set) - total)\n",
    "    return coef\n",
    "\n",
    "def print_jaccard_coef(data, c0, c1, stop_words=None):\n",
    "    class0 = [data[i] for i in np.where(twenty_train.target == c0)[0]]\n",
    "    class1 = [data[i] for i in np.where(twenty_train.target == c1)[0]]\n",
    "    vect0 = CountVectorizer(max_features = 10000, stop_words = stop_words).fit(class0)\n",
    "    vect1 = CountVectorizer(max_features = 10000, stop_words = stop_words).fit(class1)\n",
    "    print(f'Коэффициент Жаккара между {c0} и {c1} классами:', end='\\t')\n",
    "    print(round(calc_jaccard_coef(vect0.get_feature_names_out(), vect1.get_feature_names_out()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115afb5a",
   "metadata": {},
   "source": [
    "### Без удаления стоп слов и стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f8a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('the', 21030), ('to', 11046), ('of', 8892), ('and', 7733), ('in', 5967), ('that', 5745), ('is', 5640), ('it', 4248), ('for', 3630), ('you', 3614), ('this', 3047), ('on', 2910), ('be', 2610), ('are', 2540), ('have', 2415), ('not', 2353), ('with', 2243), ('as', 2143), ('or', 2055), ('if', 1918)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 0 класса:\n",
      "[('the', 6750), ('to', 3486), ('and', 2447), ('of', 2296), ('is', 2269), ('in', 1848), ('for', 1394), ('it', 1261), ('on', 1141), ('that', 1130), ('this', 1125), ('you', 1066), ('be', 956), ('are', 893), ('if', 836), ('with', 821), ('or', 778), ('can', 756), ('an', 730), ('not', 699)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 1 класса:\n",
      "[('the', 6698), ('to', 3315), ('of', 3223), ('and', 2402), ('in', 1937), ('that', 1795), ('is', 1494), ('it', 1194), ('you', 1144), ('for', 1020), ('this', 844), ('have', 769), ('be', 761), ('on', 747), ('or', 733), ('not', 730), ('they', 730), ('are', 713), ('as', 701), ('by', 691)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 2 класса:\n",
      "[('the', 7582), ('to', 4245), ('of', 3373), ('and', 2884), ('that', 2820), ('in', 2182), ('is', 1877), ('it', 1793), ('you', 1404), ('for', 1216), ('this', 1078), ('we', 1028), ('on', 1022), ('have', 1000), ('are', 934), ('not', 924), ('be', 893), ('as', 828), ('with', 815), ('they', 781)]\n",
      "\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.192\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.201\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.293\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=None); print()\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=0); print()\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=1); print()\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=2); print()\n",
    "\n",
    "print_jaccard_coef(twenty_train.data, 0, 1)\n",
    "print_jaccard_coef(twenty_train.data, 0, 2)\n",
    "print_jaccard_coef(twenty_train.data, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ab2c9",
   "metadata": {},
   "source": [
    "### С удалением стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4b92a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('people', 964), ('file', 886), ('don', 744), ('use', 723), ('like', 656), ('know', 628), ('gun', 621), ('think', 618), ('just', 608), ('window', 575), ('time', 544), ('mr', 529), ('program', 529), ('make', 493), ('edu', 484), ('president', 468), ('does', 465), ('government', 456), ('new', 444), ('right', 414)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 0 класса:\n",
      "[('file', 579), ('window', 571), ('use', 459), ('program', 412), ('server', 385), ('edu', 377), ('motif', 356), ('widget', 354), ('entry', 351), ('output', 315), ('available', 306), ('com', 274), ('set', 274), ('using', 269), ('mit', 254), ('application', 253), ('like', 247), ('information', 245), ('sun', 240), ('does', 232)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 1 класса:\n",
      "[('gun', 598), ('people', 392), ('file', 298), ('guns', 285), ('don', 255), ('firearms', 229), ('law', 206), ('right', 200), ('just', 190), ('like', 190), ('weapons', 190), ('control', 185), ('think', 184), ('government', 174), ('use', 171), ('know', 161), ('crime', 157), ('time', 157), ('state', 150), ('make', 142)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 2 класса:\n",
      "[('people', 498), ('mr', 433), ('president', 416), ('think', 374), ('don', 353), ('stephanopoulos', 345), ('know', 304), ('government', 281), ('just', 270), ('going', 263), ('like', 219), ('make', 217), ('time', 216), ('new', 204), ('said', 201), ('did', 189), ('say', 177), ('jobs', 175), ('ve', 169), ('ll', 165)]\n",
      "\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.18\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.19\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.28\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=None, stop_words='english'); print()\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=0, stop_words='english'); print()\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=1, stop_words='english'); print()\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=2, stop_words='english'); print()\n",
    "\n",
    "print_jaccard_coef(twenty_train.data, 0, 1, stop_words='english')\n",
    "print_jaccard_coef(twenty_train.data, 0, 2, stop_words='english')\n",
    "print_jaccard_coef(twenty_train.data, 1, 2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ee6a5",
   "metadata": {},
   "source": [
    "### Стоп-слова + смемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ce57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('thi', 3049), ('wa', 1655), ('use', 1557), ('file', 1117), ('ha', 1072), ('peopl', 965), ('ani', 958), ('gun', 915), ('like', 783), ('program', 776), ('window', 730), ('make', 698), ('work', 689), ('doe', 687), ('know', 677), ('think', 666), ('time', 663), ('state', 650), ('just', 608), ('right', 596)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 0 класса:\n",
      "[('file', 579), ('window', 571), ('use', 459), ('program', 412), ('server', 385), ('edu', 377), ('motif', 356), ('widget', 354), ('entry', 351), ('output', 315), ('available', 306), ('com', 274), ('set', 274), ('using', 269), ('mit', 254), ('application', 253), ('like', 247), ('information', 245), ('sun', 240), ('does', 232)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 1 класса:\n",
      "[('gun', 598), ('people', 392), ('file', 298), ('guns', 285), ('don', 255), ('firearms', 229), ('law', 206), ('right', 200), ('just', 190), ('like', 190), ('weapons', 190), ('control', 185), ('think', 184), ('government', 174), ('use', 171), ('know', 161), ('crime', 157), ('time', 157), ('state', 150), ('make', 142)]\n",
      "\n",
      "Первые 20 наиболее частотных слов 2 класса:\n",
      "[('people', 498), ('mr', 433), ('president', 416), ('think', 374), ('don', 353), ('stephanopoulos', 345), ('know', 304), ('government', 281), ('just', 270), ('going', 263), ('like', 219), ('make', 217), ('time', 216), ('new', 204), ('said', 201), ('did', 189), ('say', 177), ('jobs', 175), ('ve', 169), ('ll', 165)]\n",
      "\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.175\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.194\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.276\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(stem_train, stem_test, class_num=None, stop_words='english'); print()\n",
    "print_frequency_words(stem_train, stem_test, class_num=0, stop_words='english'); print()\n",
    "print_frequency_words(stem_train, stem_test, class_num=1, stop_words='english'); print()\n",
    "print_frequency_words(stem_train, stem_test, class_num=2, stop_words='english'); print()\n",
    "\n",
    "print_jaccard_coef(stem_train, 0, 1, stop_words='english')\n",
    "print_jaccard_coef(stem_train, 0, 2, stop_words='english')\n",
    "print_jaccard_coef(stem_train, 1, 2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055aed6",
   "metadata": {},
   "source": [
    "# Взвешивание tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a489f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_matrix_train = tfidf.fit_transform(stem_train).toarray()\n",
    "terms_train = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514afb48",
   "metadata": {},
   "source": [
    "# П7. Pipeline. Классификация с использованием наивного байесовского метода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b7a32",
   "metadata": {},
   "source": [
    "## Код для быстрой записи данных в таблицы (к лабе не относится)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f189a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "class Table():\n",
    "    def __init__(self, title=None, data=None, headlines=[\"Precision\", \"Recall\", \"F1-score\", \"Accuracy\"],\n",
    "                 indexes=[\"Стемминг отсутствует\", \"Стемминг есть\",\n",
    "                          \"Стоп-слова есть\", \"Стоп-слова отсечены\",\n",
    "                       \"Взвешивание Count\", \"Взвешивание TF\", \"Взвешивание TF-IDF\",\n",
    "                       \"max_features 100\", \"max_features 1000\", \"max_features 2000\", \"max_features 5000\", \"max_features 10000\"],\n",
    "                weighted=True):\n",
    "        \n",
    "        self.headlines = headlines\n",
    "        self.dataFrame = pd.DataFrame(columns=headlines, index=indexes)\n",
    "        if weighted:\n",
    "            self.dataFrame.columns = pd.MultiIndex.from_tuples(\n",
    "                tuple(zip(('Weighted Avg', 'Weighted Avg', 'Weighted Avg', 'Weighted Avg'), self.dataFrame.columns)))\n",
    "        \n",
    "    def fill_row(self, row, data=[], pred='', target='', roc_auc=None):\n",
    "        \"\"\"\n",
    "        Заполняет строки исходя из дефолта\n",
    "        row - row index\n",
    "        pred='' и target='' - полученные классификатором значения и целевые значения\n",
    "        Заполняется значениями Weighted Avg\"\"\"\n",
    "        data = []\n",
    "        data.append(round(precision_score(target, pred, average='weighted'), 3))\n",
    "        data.append(round(recall_score(target, pred, average='weighted'), 3))\n",
    "        data.append(round(f1_score(target, pred, average='weighted'), 3))\n",
    "        data.append(round(accuracy_score(target, pred,), 3))\n",
    "        for i in range(len(self.headlines)):\n",
    "            self.dataFrame.loc[row][i] = data[i]   \n",
    "        return self.dataFrame.loc[row]\n",
    "            \n",
    "    def table(self):\n",
    "        return self.dataFrame\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.dataFrame}'\n",
    "    \n",
    "result_table = Table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d73a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"+++++++++++++++++++++++++++++++++++++++++++\"\n",
    "# result_table.table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542f366",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e247d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def multiNB_pipeline(x_train, y_train, x_test, stop_words=None, max_features=1000, use_tf=True, use_idf=True):\n",
    "    \"\"\"Дофига умная функция классификации\n",
    "    use_tf - использовать TfidfTransformer или нет\n",
    "    \"\"\"\n",
    "\n",
    "    if use_tf:\n",
    "        text_clf = Pipeline([('vect', CountVectorizer(max_features=max_features, stop_words=stop_words)),\n",
    "                            ('tfidf', TfidfTransformer(use_idf=use_idf)),\n",
    "                            ('clf', MultinomialNB()),])   \n",
    "    else:\n",
    "        text_clf = Pipeline([('vect', CountVectorizer(max_features=max_features, stop_words=stop_words)),\n",
    "                        ('clf', MultinomialNB()),])\n",
    "    \n",
    "    text_clf = text_clf.fit(x_train, y_train)\n",
    "    prediction = text_clf.predict(x_test)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071911b",
   "metadata": {},
   "source": [
    "## Поиск наилучших параметров классификации\n",
    " *По очереди перебираем параметры, оставляя лучшие*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed7c29",
   "metadata": {},
   "source": [
    "### Наличие \\ отсутствие стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07d73a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.756\n",
      "              Recall       0.746\n",
      "              F1-score     0.749\n",
      "              Accuracy     0.746\n",
      "Name: Стемминг отсутствует, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.763\n",
      "              Recall       0.756\n",
      "              F1-score     0.758\n",
      "              Accuracy     0.756\n",
      "Name: Стемминг есть, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = multiNB_pipeline(twenty_train.data, twenty_train.target, twenty_test.data, use_tf=False)\n",
    "print(delimiter, result_table.fill_row(row='Стемминг отсутствует', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, use_tf=False)\n",
    "print(delimiter, result_table.fill_row(row='Стемминг есть', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7f1a3",
   "metadata": {},
   "source": [
    "### Отсечение \\ не отсечение стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbefedc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.763\n",
      "              Recall       0.756\n",
      "              F1-score     0.758\n",
      "              Accuracy     0.756\n",
      "Name: Стоп-слова есть, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.806\n",
      "              Recall       0.781\n",
      "              F1-score     0.787\n",
      "              Accuracy     0.781\n",
      "Name: Стоп-слова отсечены, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, use_tf=False)\n",
    "print(delimiter, result_table.fill_row(row='Стоп-слова есть', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', use_tf=False)\n",
    "print(delimiter, result_table.fill_row(row='Стоп-слова отсечены', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d1cca",
   "metadata": {},
   "source": [
    "### Взвешивание: Count, TF, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "756322a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.806\n",
      "              Recall       0.781\n",
      "              F1-score     0.787\n",
      "              Accuracy     0.781\n",
      "Name: Взвешивание Count, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.822\n",
      "              Recall       0.784\n",
      "              F1-score     0.793\n",
      "              Accuracy     0.784\n",
      "Name: Взвешивание TF, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.831\n",
      "              Recall       0.792\n",
      "              F1-score     0.802\n",
      "              Accuracy     0.792\n",
      "Name: Взвешивание TF-IDF, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', use_tf=False)\n",
    "print(delimiter, result_table.fill_row(row='Взвешивание Count', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', use_tf=True, use_idf=False)\n",
    "print(delimiter, result_table.fill_row(row='Взвешивание TF', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', use_tf=True, use_idf=True)\n",
    "print(delimiter, result_table.fill_row(row='Взвешивание TF-IDF', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db47d154",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.709\n",
      "              Recall       0.685\n",
      "              F1-score     0.694\n",
      "              Accuracy     0.685\n",
      "Name: max_features 100, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.831\n",
      "              Recall       0.792\n",
      "              F1-score     0.802\n",
      "              Accuracy     0.792\n",
      "Name: max_features 1000, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.849\n",
      "              Recall       0.805\n",
      "              F1-score     0.815\n",
      "              Accuracy     0.805\n",
      "Name: max_features 2000, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.861\n",
      "              Recall       0.812\n",
      "              F1-score     0.822\n",
      "              Accuracy     0.812\n",
      "Name: max_features 5000, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "Weighted Avg  Precision    0.875\n",
      "              Recall       0.811\n",
      "              F1-score     0.824\n",
      "              Accuracy     0.811\n",
      "Name: max_features 10000, dtype: object\n",
      "+++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', max_features=100)\n",
    "print(delimiter, result_table.fill_row(row='max_features 100', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', max_features=1000)\n",
    "print(delimiter, result_table.fill_row(row='max_features 1000', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', max_features=2000)\n",
    "print(delimiter, result_table.fill_row(row='max_features 2000', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', max_features=5000)\n",
    "print(delimiter, result_table.fill_row(row='max_features 5000', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')\n",
    "\n",
    "prediction = multiNB_pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', max_features=10000)\n",
    "print(delimiter, result_table.fill_row(row='max_features 10000', pred=prediction, target=twenty_test.target), delimiter+'\\n', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234be7f",
   "metadata": {},
   "source": [
    "# П9. Выводы\n",
    "В офшоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "989b9a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Weighted Avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Стемминг отсутствует</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Стемминг есть</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Стоп-слова есть</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Стоп-слова отсечены</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Взвешивание Count</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Взвешивание TF</th>\n",
       "      <td>0.822</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Взвешивание TF-IDF</th>\n",
       "      <td>0.831</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features 100</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features 1000</th>\n",
       "      <td>0.831</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features 2000</th>\n",
       "      <td>0.849</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features 5000</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features 10000</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Weighted Avg                         \n",
       "                        Precision Recall F1-score Accuracy\n",
       "Стемминг отсутствует        0.756  0.746    0.749    0.746\n",
       "Стемминг есть               0.763  0.756    0.758    0.756\n",
       "Стоп-слова есть             0.763  0.756    0.758    0.756\n",
       "Стоп-слова отсечены         0.806  0.781    0.787    0.781\n",
       "Взвешивание Count           0.806  0.781    0.787    0.781\n",
       "Взвешивание TF              0.822  0.784    0.793    0.784\n",
       "Взвешивание TF-IDF          0.831  0.792    0.802    0.792\n",
       "max_features 100            0.709  0.685    0.694    0.685\n",
       "max_features 1000           0.831  0.792    0.802    0.792\n",
       "max_features 2000           0.849  0.805    0.815    0.805\n",
       "max_features 5000           0.861  0.812    0.822    0.812\n",
       "max_features 10000          0.875  0.811    0.824    0.811"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result_table.table())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2318133",
   "metadata": {},
   "source": [
    "### Эмперическим путем последовательно выявили лучшие параметры для классификации\n",
    "\n",
    "- Отсечением стоп-слов убираем неинформативные признаки\n",
    "- Стемминг служит для улучшения информативности признаков, т.к. в словах учитывается только основа слова. Это группирует слова с похожими значениями\n",
    "- Далее взвешивание. Count - простой подсчет терминов в документе, TF - вес термина в конкретном документе, TF-IDF - мера, для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Такой способ __(TF-IDF)__ позволяет лучше подсчитывать важность термина, при этом учитывая всю выборку, за счет этого метод даёт лучшие результаты.\n",
    "- При изменении количества используемых терминов (max_features), выяснили, что увеличение числа признаков дает лучшие результаты классификации, хотя однозначно не ясно, лучше 5000 или 10000 признаков. Однако надо учитывать, что слишком большая размерность может привести к временным трудностям в работе алгоритма. Так же стоит учитывать информативность признаков, как слова с большой частотой встречаемости, так и крайне редкие слова могут не давать улучшения качества классификации или ухудшать его."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
